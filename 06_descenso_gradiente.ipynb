{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada de una función\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La derivada de una función $ f(x) $ se define como el límite del cociente incremental cuando el incremento tiende a cero. Matemáticamente, se expresa como:\n",
    "\n",
    "###  $ f'(x) = \\lim_{{h \\to 0}} \\frac{f(x+h) - f(x)}{h} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada(f, x, h=1e-6):\n",
    "    diferencia = (f(x+h)-f(x))/h\n",
    "    return diferencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la función $ f(x) = (x-3)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x-3)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación geométrica\n",
    "\n",
    "#### - Geometrícamente, la derivada de una función en un punto es la pendiente de la tangente a la curva de esa función en dicho punto.\n",
    "#### - La derivada describe cómo de inclinada está la curva en un punto específico.\n",
    "#### -  Esto se visualiza como una recta que toca la curva en un solo punto (la tangente) y cuya pendiente es el valor de la derivada en ese punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función f(x) = (x - 3)^2\n",
    "def f(x):\n",
    "    return (x - 3)**2\n",
    "\n",
    "# Derivada de la función f(x) = 2 * (x - 3)\n",
    "def df(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Puntos donde se evaluará la derivada\n",
    "points = [1, 2, 3]\n",
    "\n",
    "# Crear un rango de valores de x\n",
    "x = np.linspace(0, 6, 100)\n",
    "y = f(x)\n",
    "\n",
    "# Crear la gráfica de la función\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label=r'$(x - 3)^2$', color='blue')\n",
    "\n",
    "# Calcular y graficar las derivadas en los puntos dados\n",
    "for p in points:\n",
    "    slope = df(p)\n",
    "    tangent_line = slope * (x - p) + f(p)\n",
    "    plt.plot(x, tangent_line, linestyle='--', label=f'Derivada en x={p}', alpha=0.7)\n",
    "\n",
    "    # Graficar los puntos donde se calcula la derivada\n",
    "    plt.scatter(p, f(p), color='red', zorder=5)\n",
    "    plt.text(p, f(p) + 0.5, f'({p}, {f(p)})', fontsize=12, ha='center')\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.title(r'Gráfica de $(x - 3)^2$ y sus derivadas en puntos específicos', fontsize=14)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$f(x) = (x - 3)^2$')\n",
    "plt.axhline(0, color='black',linewidth=0.5)\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argumento mínimo de la función: $ f(x) = (x-3)^2 $\n",
    "\n",
    "### El **argumento mínimo** de una función es el valor de la variable independiente que hace que la función alcance su valor más pequeño (mínimo). \n",
    "\n",
    "### Si se tiene una función $ f(x) $, el argumento mínimo se denota por:\n",
    "\n",
    "### $$ \\arg \\min_{x} f(x) $$\n",
    "\n",
    "### Este símbolo significa _el valor de  $x$  que minimiza $f(x)$._ \n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "### Si se tiene la función cuadrática $ f(x) = (x - 3)^2 $, el argumento mínimo es:\n",
    "\n",
    "### $$ \\arg \\min_{x} (x - 3)^2 = 3 $$ \n",
    "\n",
    "### En $x = 3 $, la función alcanza su valor mínimo, $ f(3) = 0 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función f(x) = (x - 3)^2\n",
    "def f(x):\n",
    "    return (x - 3)**2\n",
    "\n",
    "# Generar valores de x para graficar\n",
    "x = np.linspace(0, 6, 400)\n",
    "y = f(x)\n",
    "\n",
    "# Punto mínimo en x = 3\n",
    "x_min = 3   \n",
    "y_min = f(x_min)\n",
    "\n",
    "# Graficar la función\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label=r'$f(x) = (x - 3)^2$', color='b')\n",
    "plt.scatter(x_min, y_min, color='r', zorder=5, label='Argumento mínimo (x=3)')\n",
    "plt.axvline(x=x_min, linestyle='--', color='gray', label='x = 3')\n",
    "\n",
    "# Configuración del gráfico\n",
    "plt.title(r'Función $f(x) = (x - 3)^2$ y su argumento mínimo', fontsize=14)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de la derivada en el punto  $x_0=2$ de la función $f(x) = (x-3)^2 $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aproximación de la derivada en el punto x0=2\n",
    "derivada(f, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso de la derivada\n",
    "### Se elige $x_0$ aleatorio\n",
    "### $$ x_{n+1} = x_{n} - \\eta f'(x_n) $$\n",
    "\n",
    "### Donde:\n",
    "\n",
    "### - $\\eta$ es la tasa de aprendizaje; $\\eta$ > 0\n",
    "\n",
    "### - $f'(x_n)$\n",
    "\n",
    "### - $x_{n+1}$ es el valor actualizado de $x_n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# punto inicial aleatorio\n",
    "x = np.random.randint(-10,10)\n",
    "print(x)\n",
    "Loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "print(f\"punto inicial: {x}\")\n",
    "for epoch in range(1000):\n",
    "    x = x - learning_rate * derivada(f, x)\n",
    "    Loss.append(f(x))\n",
    "    print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Loss)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso del Gradiente\n",
    "\n",
    "### $$ x  = x - \\eta \\nabla f(x) $$\n",
    "\n",
    "### Donde:\n",
    "\n",
    "### - $\\eta$ es la tasa de aprendizaje; $\\eta$ > 0\n",
    "\n",
    "### - $\\nabla f(x)$ es el gradiente de la función\n",
    "\n",
    "### - $x$ es un vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### El **descenso del gradiente** es un algoritmo de optimización iterativo utilizado para encontrar los mínimos de una función. \n",
    "- ### En el caso de una función de dos variables, el descenso del gradiente sigue el mismo principio básico que en una variable, pero ahora se mueve en un espacio bidimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico del descenso del gradiente en 2D\n",
    "\n",
    "Vamos a graficar un ejemplo del descenso del gradiente en 2D para una función cuadrática como $ f(x, y) = x^2 + y^2 $.\n",
    "\n",
    "### $$ \\arg \\min_{(x,y)} f(x,y) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir los rangos de x e y\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Definir la función f(x, y) = x^2 + y^2\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "# Crear la figura\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Graficar el contorno de la función\n",
    "contour = ax.contourf(X, Y, Z, cmap='viridis')\n",
    "fig.colorbar(contour)\n",
    "\n",
    "# Resaltar el mínimo de la función\n",
    "min_x = 0\n",
    "min_y = 0\n",
    "ax.scatter(min_x, min_y, color='red', label='Mínimo')\n",
    "\n",
    "# Etiquetas\n",
    "ax.set_title('Gráfico de $f(x, y) = x^2 + y^2$ con el mínimo resaltado')\n",
    "ax.set_xlabel('Eje X')\n",
    "ax.set_ylabel('Eje Y')\n",
    "\n",
    "# Añadir leyenda\n",
    "ax.legend()\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Definir los rangos de x e y\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Definir la función f(x, y) = x^2 + y^2\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "# Crear la figura y los ejes 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Graficar la superficie\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "# Etiquetas\n",
    "ax.set_title('Gráfico de $f(x, y) = x^2 + y^2$')\n",
    "ax.set_xlabel('Eje X')\n",
    "ax.set_ylabel('Eje Y')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso del descenso del gradiente\n",
    "\n",
    "#### Dada una función $ f(x, y)$, el gradiente $ \\nabla f(x, y) $ es un vector que contiene las derivadas parciales de $ f $ respecto a $ x $ y $ y $:\n",
    "\n",
    "####  $$ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) $$\n",
    "\n",
    "####  El descenso del gradiente se basa en actualizar los valores de $ x $ y $ y $ en la dirección opuesta al gradiente, es decir, en la dirección de mayor descenso. La actualización se realiza de la siguiente manera:\n",
    "\n",
    "####  $$ x_{\\text{n+1}} = x_{\\text{n}} - \\eta \\frac{\\partial f}{\\partial x} $$\n",
    "\n",
    "####  $$ y_{\\text{n+1}} = y_{\\text{n}} - \\eta \\frac{\\partial f}{\\partial y} $$\n",
    "\n",
    "#### Donde:\n",
    "#### - $ \\eta $ es la **tasa de aprendizaje**, un valor que determina el tamaño del paso en cada iteración.\n",
    "#### - $  \\frac{\\partial f}{\\partial x} $ y $ \\frac{\\partial f}{\\partial y} $  son las derivadas parciales de $ f $ respecto a $ x $ y $ y $, respectivamente.\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "#### Consideremos la función:\n",
    "\n",
    "####  $$ f(x, y) = x^2 + y^2 $$\n",
    "\n",
    "#### El gradiente de $ f(x, y) $ es:\n",
    "\n",
    "####  $$ \\nabla f(x, y) = \\left( 2x, 2y \\right) $$\n",
    "\n",
    "####  Para aplicar el descenso del gradiente:\n",
    "\n",
    "####  1. Elegimos un punto inicial $ (x_0, y_0) $, por ejemplo, $ (x_0, y_0) = (3, 4) $.\n",
    "####  2. Calculamos el gradiente $ \\nabla f(x_0, y_0) = (6, 8) $.\n",
    "####  3. Actualizamos $ x $ y $ y $ usando una tasa de aprendizaje $ \\eta  = 0.1 $:\n",
    "####  $$  x_1 = 3 - 0.1 \\times 6 = 2.4 $$\n",
    "####  $$  y_1 = 4 - 0.1 \\times 8 = 3.2 $$\n",
    "####  4. Repetimos el proceso hasta que el gradiente se acerque a cero, lo que indica que hemos llegado a un mínimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso del gradiente con diferenciación numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función f(x, y) = x^2 + y^2 y su gradiente\n",
    "def f(vector):\n",
    "    x = vector[0]\n",
    "    y = vector[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "def derivada_parcial(f, indx, vector, h=1e-5):\n",
    "    _vec = vector.copy()\n",
    "    _vec[indx] += h\n",
    "    diferencia = (f(_vec) - f(vector))/h\n",
    "    return diferencia\n",
    "\n",
    "# def grad_f(x, y):\n",
    "#     return np.array([2*x, 2*y])\n",
    "\n",
    "def grad_f(vector):\n",
    "    grad =[]\n",
    "    for k in range(vector.shape[0]):\n",
    "       grad.append(derivada_parcial(f, k, vector))\n",
    "    return np.array(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiente en el punto x, y   f(x) = x^2 + y^2\n",
    "x, y = 6, 8\n",
    "vector = np.array([x, y], dtype=np.float64)\n",
    "grad = grad_f(vector)\n",
    "print(f\"punto({x},{y})\")\n",
    "print(f\"grad:\", grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del descenso del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros del descenso de gradiente\n",
    "learning_rate = 0.1\n",
    "num_pasos= 5\n",
    "\n",
    "# Inicializar el punto\n",
    "x, y = 6, 8\n",
    "vector = np.array([x, y], dtype=np.float64)\n",
    "points = [(vector)]  # Lista para almacenar los puntos visitados\n",
    "\n",
    "# Realizar el descenso de gradiente\n",
    "for i in range(num_pasos):\n",
    "    # print(vector)\n",
    "    print(\"punto (x,y):\", vector)\n",
    "    gradient = grad_f(vector)\n",
    "    print(\"grad:\", gradient)\n",
    "    vector[0] -= learning_rate * gradient[0]\n",
    "    vector[1] -= learning_rate * gradient[1]\n",
    "    points.append((vector[0], vector[1]))\n",
    "\n",
    "# Convertir los puntos a arrays\n",
    "points = np.array(points)\n",
    "X_steps, Y_steps = points[:, 0], points[:, 1]\n",
    "\n",
    "# Definir los rangos de x e y\n",
    "x_range = np.linspace(-10, 10, 100)\n",
    "y_range = np.linspace(-10, 10, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Calcular Z = f(x, y)\n",
    "\n",
    "Z = f(np.array((X, Y)))\n",
    "\n",
    "# Crear la gráfica de contornos de la función\n",
    "fig, ax = plt.subplots()\n",
    "contour = ax.contour(X, Y, Z, levels=np.logspace(0, 3, 35), cmap='viridis')\n",
    "fig.colorbar(contour)\n",
    "\n",
    "# Graficar los pasos del gradiente descendente\n",
    "ax.plot(X_steps, Y_steps, 'ro-', label='Pasos del gradiente descendente')\n",
    "\n",
    "# Resaltar el punto inicial y final\n",
    "ax.scatter(6, 8, color='blue', label='Punto inicial')\n",
    "ax.scatter(0, 0, color='green', label='Punto final (mínimo)')\n",
    "\n",
    "# Etiquetas\n",
    "ax.set_title('Descenso de Gradiente para $f(x, y) = x^2 + y^2$')\n",
    "ax.set_xlabel('Eje X')\n",
    "ax.set_ylabel('Eje Y')\n",
    "\n",
    "# Añadir leyenda\n",
    "ax.legend()\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mínimos Locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir el rango de valores para x\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Definir la función con mínimos locales más pronunciados\n",
    "y = np.sin(1.8 * x) + 0.1 * x**2 \n",
    "\n",
    "# Graficar la función\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label=r'$y = \\sin(3x) + 0.1x^2$')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.title(\"Función con Mínimos Locales Pronunciados\", fontsize=16)\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14)\n",
    "\n",
    "# Añadir una cuadrícula para facilitar la lectura de los mínimos\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar la gráfica con márgenes ajustados\n",
    "plt.savefig(\"./figs/fig-minimos_locales.png\", bbox_inches='tight',pad_inches=0, dpi=300)  # Guardar con resolución de 300 dpi\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
