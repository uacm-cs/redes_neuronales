{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Pytorch\n",
    "\n",
    "<img src=\"figs/fig-pytorch.png\" width=\"30%\">\n",
    "\n",
    "[https://pytorch.org](https://pytorch.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **PyTorch** es una biblioteca de aprendizaje automático de código abierto desarrollada por Meta AI.\n",
    "###  - **Tensores**: Principal bloque de construcción (arreglos multi-dimensionales, no estrictamente el mismo significado que en matemáticas), similar a NumPy, con soporte para GPUs.\n",
    "###  - **Autograd**: Sistema de diferenciación automática para el cálculo de los gradientes y facilitar el entrenamiento de modelos.\n",
    "###  - **Ecosistema**: Incluye bibliotecas para visión por computadora, procesamiento de lenguaje natural y audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación \n",
    "\n",
    "###  <span style=\"color:cyan\"> Windows </span>\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "##### CUDA - Compute Unified Device Architecture (https://developer.nvidia.com/cuda-toolkit)\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### PIP\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "##### CUDA - Compute Unified Device Architecture (https://developer.nvidia.com/cuda-toolkit)\n",
    "\n",
    "```bash\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  <span style=\"color:cyan\">  Mac OS </span>\n",
    "\n",
    "### Anaconda\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision -c pytorch\n",
    "```\n",
    "\n",
    "### PIP\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de la instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paquete: torch\n",
    "[https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html)\n",
    "\n",
    "- ### El paquete Torch contiene estructuras de datos para tensores multidimensionales y define operaciones matemáticas sobre estos tensores\n",
    "- ### Tiene una contraparte CUDA, que le permite ejecutar sus cálculos de tensores en una GPU NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción a los tensores\n",
    "\n",
    "###  Tensores\n",
    "\n",
    "- #### Los tensores son una estructura de datos similar a las matrices y arreglos.\n",
    " - #### En PyTorch, se usan tensores para codificar las entradas y salidas de un modelo, así como los parámetros del modelo.\n",
    "\n",
    "- #### Los tensores son similares a los ndarrays de NumPy, excepto que los tensores pueden ejecutarse en GPU u otros aceleradores de hardware.\n",
    "\n",
    " - ####  Los tensores están optimizados para la diferenciación automática (Autograd).\n",
    "\n",
    " - #### Su función es representar los datos de forma numérica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Creación de un tensor desde datos\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A partir de una matriz NumPy\n",
    "# Se pueden crear tensores a partir de matrices NumPy (y viceversa).\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El nuevo tensor conserva las propiedades (forma, tipo de datos) del tensor del argumento\n",
    "\n",
    "x_ones = torch.ones_like(x_data) # conserva las propiedades de x_data\n",
    "print(f\"Tensor de unos: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float32) # anula el tipo de datos de x_data\n",
    "print(f\"Tensor aleatorio: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3)\n",
    "# random\n",
    "rand_tensor = torch.rand(shape)\n",
    "# unos\n",
    "ones_tensor = torch.ones(shape)\n",
    "# ceros\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Tensor aleatorio: \\n {rand_tensor} \\n\")\n",
    "print(f\"Tensor de unos: \\n {ones_tensor} \\n\")\n",
    "print(f\"Tensor de ceros: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Forma del tensor: {tensor.shape}\")\n",
    "print(f\"Tipo de datos del tensor: {tensor.dtype}\")\n",
    "print(f\"El tensor del dispositivo se almacena en: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando tensores en GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movemos nuestro tensor a la GPU si está disponible\n",
    "print(\"GPU disponible: \", torch.cuda.is_available())\n",
    "\n",
    "# Por default los tensores se crean en el cpu\n",
    "tensor  = torch.rand((3,4))\n",
    "print(\"tensor en: \", tensor.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():  # GPU\n",
    "    print(\"Moviendo tensor a GPU: \", torch.cuda.is_available())\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    tensor2 = torch.rand((3,4), device=\"cuda\")\n",
    "    print(\"Creando tensor2 en GPU: \", tensor2)\n",
    "\n",
    "if torch.backends.mps.is_available():  #GPU en MAC\n",
    "    print(\"Moviendo tensor a GPU Mac: \", torch.backends.mps.is_available())\n",
    "    tensor = tensor.to(\"mps\")\n",
    "    tensor2 = torch.rand((3,4), device=\"mps\")\n",
    "    print(\"Creando tensor2 en GPU Mac: \", tensor2)\n",
    "\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexamiento y slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexación y segmentación estándar de tipo numpy:\n",
    "\n",
    "tensor = torch.linspace(1, 16, 16).reshape(4,4)\n",
    "print(tensor)\n",
    "print(f\"Primera fila: {tensor[0]}\")\n",
    "print(f\"Primera columna: {tensor[:, 0]}\")\n",
    "print(f\"Última columna: {tensor[:, -1]}\")\n",
    "# colocar ceros a la columna 1 \n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones aritméticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto calcula la multiplicación de matrices entre dos tensores. y1, y2, y3 tendrán el mismo valor\n",
    "# ``tensor.T`` devuelve la transpuesta de un tensor\n",
    "\n",
    "tensor = torch.linspace(1, 6, 6).reshape(2,3)\n",
    "\n",
    "print(tensor)\n",
    "y1 = tensor @ tensor.T\n",
    "print(f\"y1:\\n{y1}\\n\")\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(f\"y2:\\n{y2}\\n\")\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "print(f\"y3:\\n{y3}\\n\")\n",
    "\n",
    "# Esto calcula el producto elemento por elemento. z1, z2, z3 tendrán el mismo valor\n",
    "z1 = tensor * tensor\n",
    "print(f\"z1:\\n{z1}\\n\")\n",
    "\n",
    "z2 = tensor.mul(tensor)\n",
    "print(f\"z2:\\n{z2}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de imagen\n",
    "- ### Por ejemplo, se podría representar una imagen como un tensor con forma [3, 200, 200]\n",
    "- ### lo que significaría [canales_de_color, altura, ancho], dado que la imagen tiene 3 canales de color (red, green, blue), una altura de 200 píxeles y un ancho de 200 píxeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tensor_image = torch.randn((3, 200, 200) , dtype=torch.float32)\n",
    "print(tensor_image.shape)\n",
    "#nn.init.xavier_uniform_(tensor_image, gain=18)\n",
    "\n",
    "#Crea un nuevo vector con los mismo datos, con diferente forma (shape)  forma para visualizar 200x200x3\n",
    "tensor = tensor_image.view(tensor_image.shape[1], tensor_image.shape[2], tensor_image.shape[0])\n",
    "plt.imshow(tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de capas lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa lineal\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "#Estable la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#creación de la capa lineal de 3x2: 3 entradas y 2 neuronas de salida\n",
    "linear_layer = nn.Linear(in_features=3,out_features=2)\n",
    "\n",
    "# parámetro donde se guardan los pesos\n",
    "print(\"weight:\", linear_layer.weight)\n",
    "\n",
    "# El bias se crea automáticamente de acuerdo al números de neuronas\n",
    "print(\"bias:\", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción de datos a la capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definición de entradas\n",
    "\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "#Aplicación de la capa lineal en Pytorch\n",
    "z_c1 = linear_layer(x)\n",
    "print(\"z_c1:\", z_c1)\n",
    "\n",
    "\n",
    "print(\"\\n\\nCáluclo manual de z para las entradas de x:\")\n",
    "print(\"inputs:\", x)\n",
    "print(\"z11 : \", 0.4414*1 + 0.4792*2 -0.1353*3 + (-0.2811) )\n",
    "print(\"z12 : \", 0.5304*1 -0.1265*2 + 0.1165*3 + (0.3391) )\n",
    "\n",
    "torch.matmul(x, linear_layer.weight.T) + linear_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de funciones de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion de activación Sigmoide\n",
    "sigmoid = nn.Sigmoid()\n",
    "z = torch.tensor([5, 0, -5], dtype=torch.float32)\n",
    "a = sigmoid(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion de activación ReLU\n",
    "relu = nn.ReLU()\n",
    "\n",
    "z = torch.tensor([5, 0, -5], dtype=torch.float32)\n",
    "a = relu(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion de activación ELU\n",
    "elu = nn.ELU()\n",
    "\n",
    "z = torch.tensor([5, 0, -5], dtype=torch.float32)\n",
    "a = elu(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion de activación PReLU\n",
    "prelu = nn.PReLU()\n",
    "\n",
    "z = torch.tensor([5, 0, -5], dtype=torch.float32)\n",
    "a = prelu(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de Gradientes: AutoGrad\n",
    "\n",
    "### Torch.autograd proporciona clases y funciones que implementan la diferenciación automática de funciones con valores escalares arbitrarios.\n",
    "\n",
    "### Requiere declarar los tensores para los que se deben calcular los gradientes con la palabra clave **require_grad=True**.\n",
    "\n",
    "### **backward**  Calcular la suma de gradientes de tensores dados con respecto a las hojas del gráfico generado.\n",
    "\n",
    "### **grad**  Compute and return the sum of gradients of outputs with respect to the inputs.\n",
    "\n",
    "#### A continuación se muestra una representación visual del grado (DAG) del ejemplo siguiente. En el gráfico, las flechas apuntan en la dirección del paso hacia adelante. Los nodos representan las funciones hacia atrás de cada operación en el paso hacia adelante. Los nodos de hoja en azul representan  tensores de hojas a y b.\n",
    "\n",
    "$$ Q=3a^3 - b^2 $$\n",
    "\n",
    "$$ \\frac {\\partial Q}{\\partial a} =9a^2 $$ \n",
    "$$ \\frac {\\partial Q}{\\partial b} = - 2b $$\n",
    "<img src=\"figs/fig-dag_autograd.png\" width=\"20%\">\n",
    "\n",
    "\n",
    "Cuando se llama a **.backward()** en Q, autograd calcula estos gradientes y los almacena en el atributo **.grad** de los tensores respectivos.\n",
    "\n",
    "Se requiere pasar explícitamente un argumento **gradient** en **Q.backward()** porque es un vector. **gradient** es un tensor de la misma forma que Q y representa el gradiente de Q con respecto a sí mismo, es decir,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tensores que requieren el cálculo de los gradientes\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "# Funcion Q = a^3 - b^2\n",
    "\n",
    "Q = a**3 - b**2\n",
    "\n",
    "# Derivada de Q = 3a^2 - 2b\n",
    "# Derivada de Q respecto a 'a'  Qa = 3a^2 \n",
    "# Derivada de Q respecto a 'b'  Qb = -2b\n",
    "\n",
    "# Cálculo de los gradientes\n",
    "# Se requiere como parámetro el tamaño de las dimensiones del vector o matriz que se calcularán los gradientes\n",
    "print(torch.ones_like(b))\n",
    "Q.backward(gradient=torch.ones_like(b))\n",
    "\n",
    "print(\"Pytorch gradiente de a:\",  a.grad)\n",
    "print(\"Pytorch gradiente de b:\",  b.grad)\n",
    "print(Q.grad_fn)\n",
    "\n",
    "\n",
    "# Comparación de los gradientes\n",
    "print(\"\")\n",
    "print(\"Gradientes calculados con las derivadas parciales\")\n",
    "print(\"Gradiente respecto a 'a' :\",  3*a**2)\n",
    "print(\"Gradiente respecto a 'b' :\",  -2*b)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de la función sigmoide\n",
    "\n",
    "### $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "### Derivada de la función sigmoide\n",
    "\n",
    "### $\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradientes de la función de sigmoide\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "z = torch.tensor([5, 0, -5], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "a = sigmoid(z)\n",
    "print(\"activaciones:\", a)\n",
    "\n",
    "a.backward(torch.ones_like(z))\n",
    "\n",
    "print(\"Gradientes:\", z.grad)\n",
    "\n",
    "# Derivada símbolica de la función y=sigmoide(z): y*(1-y)\n",
    "print(\"\\nGradientes manuales\")\n",
    "\n",
    "print(a[0]*(1-a[0]))\n",
    "\n",
    "print(\"\\n\", a*(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradientes de la función de activación ReLu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "z = torch.tensor([5, 0, -5], dtype=torch.float32, requires_grad=True)\n",
    "a = relu(z)\n",
    "\n",
    "a.backward(torch.ones_like(z))\n",
    "\n",
    "print(a)\n",
    "print(\"Gradientes:\", z.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio de gradientes  1\n",
    "### Obtener los gradientes de la función f(x,y) respecto a los tensores $x$ y $y$ \n",
    "### $f(x, y) = x^2  y + \\sin(x + y) $\n",
    "\n",
    "### 1. Definir los tensores $x$=2.0 y $y$=3.0\n",
    "### 2. Usar las funciones torch.sin(z) y torch.cos(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. Definir los tensores y calcular los gradientes de los tensores x y y con la función anterior\n",
    "# 2. Comprobar manualmente que los gradientes  son los mismos obtenidos con el mecanismo de backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio de gradientes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Función de activación sigmoide\n",
    "def fn_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def fn_sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dado el siguiente tensor t apicar una función aplicar dos veces una función sigmoide a t es decir y = sigmoide(sigmoide(t)), en pasos separados, \n",
    "# es decir con variables intermedias para aplicar la  regla de la cadena\n",
    "# 1. Obtener el gradiente de t por medio de la librería PyTorch (backward)\n",
    "# 2. Comprobar que el gradiente de t es el mismo obtenido de manera símbolica derivando la función sigmoide y aplicando la regla de la cadena, esto es,  dy_dt\n",
    "# Nota1:  Para comprobar el gradiente de manera manual, hacer uso de las funciones fn_sigmoide y fn_sigmoid_derivative\n",
    "# Nota2:  El cálculo de la derivada con la fn_sigmoid_derivative considera la entrada como el parámetro x ya activado, es decir,  x = sigmoide(t)\n",
    "#         Si quiere aplicar el valor de entrada t en la dereivada use la linea comentada sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "t = torch.tensor([0.8762], requires_grad = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
