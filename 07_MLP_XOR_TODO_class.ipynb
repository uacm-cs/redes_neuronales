{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con Backpropagation para resolver el problema de la función XOR \n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "1. **Definir la arquitectura de la red**:  \n",
    "   - La red tendrá 2 entradas (los valores binarios del XOR), una capa oculta con 2 neuronas, y una neurona de salida.\n",
    "   - Usar la función de activación sigmoide en las neuronas de la capa oculta y de salida.\n",
    "   - Establecer una tasa de aprendizaje (ej. 0.5) y el número de épocas de entrenamiento.\n",
    "\n",
    "   Por ejemplo, para la capa de salida (2 neuronas en la capa oculta, 1 neurona de salida):\n",
    " $$ W^{(2)} \\in \\mathbb{R}^{1 \\times 2} $$\n",
    " $$ b^{(2)} \\in \\mathbb{R}^{1 \\times 1} $$\n",
    "\n",
    "2. **Inicializar los pesos y los sesgos**:  \n",
    "   - Inicializar los pesos de las conexiones de la capa de entrada a la capa oculta y de la capa oculta a la capa de salida, de manera aleatoria (puedes usar la inicialización Xavier).\n",
    "   - También inicializar los sesgos de cada capa.\n",
    "\n",
    "3. **Propagación hacia adelante (Forward pass)**:  \n",
    "   - Para cada entrada, multiplicar las entradas por los pesos de la capa oculta y sumar el sesgo.\n",
    "   - Aplicar la función de activación (sigmoide) para obtener las activaciones de la capa oculta.\n",
    "   - Repetir el proceso con los valores de la capa oculta para calcular la activación de la capa de salida.\n",
    "\n",
    "4. **Calcular el error**:  \n",
    "   - Calcular el error en la salida utilizando una función de error, como el Error Cuadrático Medio (MSE).\n",
    "\n",
    "5. **Backpropagation (Propagación hacia atrás)**:  \n",
    "   - Calcular los gradientes de error en la capa de salida\n",
    "   - Propagar el error hacia la capa oculta, calculando el gradiente de error en la capa oculta.\n",
    "   \n",
    "6. **Actualizar los pesos y sesgos**:  \n",
    "   - Usar los gradientes obtenidos para ajustar los pesos y los sesgos de la capa de salida y de la capa oculta utilizando el gradiente descendente.\n",
    "   \n",
    "7. **Repetir el entrenamiento**:  \n",
    "   - Repetir los pasos de forward, cálculo de error y backpropagation por el número de épocas definido hasta que el error disminuya significativamente.\n",
    "\n",
    "8. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas XOR y verificar que las salidas estén cerca de los valores esperados (0 o 1).\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # TODO: Implementar inicialización de Xavier\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al parámetro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, random_state=42):\n",
    "\n",
    "        # Establecer la semilla para la generación de números aleatorios\n",
    "        seed(42)\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el número de épocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tamaño del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # definir las capas\n",
    "        self.W1 = xavier_initialization(num_neuronas_ocultas, num_entradas)  # Pesos entre capa de entrada y capa oculta\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))   # Bias de la capa oculta\n",
    "        self.W2 = xavier_initialization(num_salidas, num_neuronas_ocultas)  # Pesos entre capa oculta y capa de salida\n",
    "        self.b2 = np.zeros((1, num_salidas)) # Bias de la capa de salida\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: implementar el forward pass\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "        self.X = X\n",
    "        self.z_c1 = 0\n",
    "        # TODO: Calcular la activación de la capa oculta usando la función sigmoide\n",
    "        self.a_c1 = 0\n",
    "        # TODO: Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "        self.z_c2 = 0\n",
    "        # TODO: Calcular la activación de la capa de salida usando la función sigmoide\n",
    "        y_pred = 0  # Activación capa de salida\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. Cálculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular el error cuadrático medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        # TODO: implementar el backward pass\n",
    "        # calcular los gradientes para la arquitectura de la figura anterior\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = 0 # Derivada del error respecto a la predicción con  N ejemplos\n",
    "        # TODO: Calcular la derivada de la activación de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = 0\n",
    "        # TODO: Calcular delta de la capa de salida\n",
    "        delta_c2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = 0\n",
    "        # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = 0\n",
    "\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = 0\n",
    "        self.dE_db2 =0\n",
    "        self.dE_dW1 =  0\n",
    "        self.dE_db1 =  0\n",
    "\n",
    "\n",
    "    def update(self):  # Ejecución de la actualización de paramámetros\n",
    "        # TODO: implementar la actualización de los pesos y el bias\n",
    "        #----------------------------------------------\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = 0\n",
    "        self.b2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 1\n",
    "        self.W1 = 0\n",
    "        self.b1 = 0\n",
    "\n",
    "    def predict(self, X):  # Predecir la categoría para datos nuevos\n",
    "        # TODO: implementar la predicción \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "        for epoch in range(self.epochs):\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                epoch_error += error    \n",
    "                self.backward() # cálculo de los gradientes\n",
    "                self.update() # actualización de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N épocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"Época {epoch}, Error batch {num_batch}: {error}\")\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                    print(f\"Época {epoch}, Error: {epoch_error/num_batch}\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo del uso para el entrenamiento\n",
    "\n",
    "# Definimos los datos de entrada para XOR\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Salidas esperadas para XOR\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "entradas = 2\n",
    "neuronas_ocultas = 2\n",
    "salidas = 1\n",
    "batch_size = 2 # Batch de 2 ejemplos # X.shape[0] # Batch de todos los ejemplos\n",
    "# batch_size = X.shape[0] # Batch de todos los ejemplos\n",
    "learning_rate = 0.5\n",
    "epochs = 10000\n",
    "clasificador = MLP_TODO(entradas, neuronas_ocultas, salidas, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
    "clasificador.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de uso para la predicción\n",
    "\n",
    "y_pred = clasificador.predict(X)\n",
    "print(\"Esperada:\\n\", y)\n",
    "print(\"Predicha:\\n\", y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
