{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con Backpropagation para resolver el problema de la función XOR \n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"30%\">\n",
    "\n",
    "\n",
    "1. **Definir la arquitectura de la red**:  \n",
    "   - La red tendrá 2 entradas (los valores binarios del XOR), una capa oculta con 2 neuronas, y una neurona de salida.\n",
    "   - Usar la función de activación sigmoide en las neuronas de la capa oculta y de salida.\n",
    "   - Establecer una tasa de aprendizaje (ej. 0.5) y el número de épocas de entrenamiento.\n",
    "\n",
    "   Por ejemplo, para la capa de salida (2 neuronas en la capa oculta, 1 neurona de salida):\n",
    " $$ W^{(2)} \\in \\mathbb{R}^{1 \\times 2} $$\n",
    " $$ b^{(2)} \\in \\mathbb{R}^{1 \\times 1} $$\n",
    "\n",
    "2. **Inicializar los pesos y los sesgos**:  \n",
    "   - Inicializar los pesos de las conexiones de la capa de entrada a la capa oculta y de la capa oculta a la capa de salida, de manera aleatoria (puedes usar la inicialización Xavier).\n",
    "   - También inicializar los sesgos de cada capa.\n",
    "\n",
    "3. **Propagación hacia adelante (Forward pass)**:  \n",
    "   - Para cada entrada, multiplicar las entradas por los pesos de la capa oculta y sumar el sesgo.\n",
    "   - Aplicar la función de activación (sigmoide) para obtener las activaciones de la capa oculta.\n",
    "   - Repetir el proceso con los valores de la capa oculta para calcular la activación de la capa de salida.\n",
    "\n",
    "4. **Calcular el error**:  \n",
    "   - Calcular el error en la salida utilizando una función de error, como el Error Cuadrático Medio (MSE).\n",
    "\n",
    "5. **Backpropagation (Propagación hacia atrás)**:  \n",
    "   - Calcular los gradientes de error en la capa de salida\n",
    "   - Propagar el error hacia la capa oculta, calculando el gradiente de error en la capa oculta.\n",
    "   \n",
    "6. **Actualizar los pesos y sesgos**:  \n",
    "   - Usar los gradientes obtenidos para ajustar los pesos y los sesgos de la capa de salida y de la capa oculta utilizando el gradiente descendente.\n",
    "   \n",
    "7. **Repetir el entrenamiento**:  \n",
    "   - Repetir los pasos de forward, cálculo de error y backpropagation por el número de épocas definido hasta que el error disminuya significativamente.\n",
    "\n",
    "8. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas XOR y verificar que las salidas estén cerca de los valores esperados (0 o 1).\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    # TODO: Implementar la función sigmoide\n",
    "    return s\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # TODO: Implementar la derivada de la función sigmoide considerar que el valor x es  sigma(x)\n",
    "    \n",
    "    return derivate\n",
    "\n",
    "# Definimos los datos de entrada para XOR\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Salidas esperadas para XOR\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "# Inicializamos los pesos y biases aleatoriamente\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "\n",
    "W1 =   # Pesos entre capa de entrada y capa oculta\n",
    "b1 =   # Bias de la capa oculta\n",
    "W2 =   # Pesos entre capa oculta y capa de salida\n",
    "b2 =   # Bias de la capa de salida\n",
    "\n",
    "# Definimos la tasa de aprendizaje\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Número de iteraciones de entrenamiento\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 1. Propagación hacia adelante (Forward pass)\n",
    "    #----------------------------------------------\n",
    "    # TODO: Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "    z_c1 = ?\n",
    "    \n",
    "    # TODO: Calcular la activación de la capa oculta usando la función sigmoide\n",
    "\n",
    "    a_c1 = ?\n",
    "\n",
    "    # TODO: Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "    z_c2 = ?\n",
    "\n",
    "    # TODO: Calcular la activación de la capa de salida usando la función sigmoide\n",
    "    y_pred = ?\n",
    "    \n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 2. Cálculo del error con MSE\n",
    "    #----------------------------------------------\n",
    "    # TODO: Calcular el error cuadrático medio (MSE)\n",
    "    \n",
    "    error = ?\n",
    "    \n",
    "    total_error = ?  # error total del batch de entrenamiento\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 3. Propagación hacia atrás (Backward pass)\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # Gradiente de la salida\n",
    "    #----------------------------------------------\n",
    "    # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "\n",
    "    dE_dy_pred = ?\n",
    "    \n",
    "    # TODO: Calcular la derivada de la activación de la salida con respecto a z_c2 \n",
    "\n",
    "    d_y_pred_d_zc2 = ?\n",
    "\n",
    "    # TODO: Calcular delta de la capa de salida\n",
    "    delta_c2 = ?\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # Gradiente en la capa oculta\n",
    "    #----------------------------------------------\n",
    "    # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "    \n",
    "    delta_c1 =  ? \n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 4. Actualización de los pesos y biases\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "    W2 =  W2 -  ? \n",
    "    b2 =  b2 - ? \n",
    "\n",
    "    # TODO: Actualizar los pesos y bias de la capa oculta\n",
    "    W1 = W1 - ?\n",
    "    b1 = b1 - ?\n",
    "\n",
    "    # Imprimir el error cada 1000 épocas\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Época {epoch}, Error: {total_error}\")\n",
    "\n",
    "\n",
    "\n",
    "# Comprobar los resultados del modelo entrenado\n",
    "# Recordar: al final del entrenamiento los parámetros de la red ya se ajustaron, es decir, las matrices Ws y Bias Bs se usan para predecir nuevos datos através de la red.\n",
    "print(\"\\nResultados finales:\")\n",
    "\n",
    "for i in range(len(X)):\n",
    "    # TODO: Realizar la propagación hacia adelante para cada entrada de prueba\n",
    "    z_hidden = ?    # Suma ponderada de la capa oculta\n",
    "    a_hidden = ?    # activación de la neurona\n",
    "    z_output = ?    # Suma ponderada de la capa de salida \n",
    "    y_pred = ?           # Predicción para el ejemplo i\n",
    "    \n",
    "    # Mostrar las predicciones\n",
    "    print(f\"Entrada: {X[i]}, Salida estimada: {y_pred}, Salida real: {y[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
