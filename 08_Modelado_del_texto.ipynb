{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esquema General de un clasificador\n",
    "\n",
    "\n",
    "<img src=\"figs/fig-diagrama-clasificador.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos para construir el clasificador\n",
    "\n",
    "## 1. Modelado del texto\n",
    "- ### 1.1 Preprocesamiento del texto: incluye remover signos, stopwords, números, es decir, datos que no aporten a la semántica del texto\n",
    "- ### 1.2 Vectorizar el texto: convertir el texto a una forma que la computadora pueda procesar\n",
    "    - ### 1.2.1 Calcular la importancia de las componentes (términos) de cada vector: \n",
    "        - #### Pesado de términos como Frecuencia de Término (TF) \n",
    "        - #### Frecuencia de Término – Frecuencia Inversa de Documento (TF-IDF)\n",
    "\n",
    "    - ### 1.3 También se puede usar una representación basada en Word Embeddings\n",
    "        - #### Glove\n",
    "        - #### Word2Vec\n",
    "        - #### BERTs\n",
    "\n",
    "##  2. Entrenar un clasificador para construir el modelo con los datos procesados\n",
    "###  Usar un clasificador como:\n",
    "- ###  Red Neuronal\n",
    "- ###  Support Vector Machine\n",
    "- ###  K-NN (K vecinos más cercanos)\n",
    "- ###  etc.\n",
    "\n",
    "##  3. Evaluar el desempeño del clasificador construido\n",
    "###  Usar métricas como:\n",
    "- ###  Precisión\n",
    "- ###  Recall\n",
    "- ###  F1-score\n",
    "\n",
    "##  4. Desplegar el modelo para su uso en un ambiente de pruebas y eventualmente en producción\n",
    "\n",
    "###  Por ejemplo:\n",
    "- ###  Aplicación WEB\n",
    "- ###  API REST\n",
    "- ###  API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de paquetes para Procesamiento de Lenguaje Natural\n",
    "\n",
    "\n",
    "### NLTK - Paquete para procesamiento de lenguaje natural: Natural Language Tool Kit\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "### spaCY - Paquete para procesamiento de lenguaje natural\n",
    "```bash\n",
    "pip install setuptools wheel\n",
    "pip install spacy==3.7.6\n",
    "```\n",
    "\n",
    "### scikit-learn - Paquete de Machine Learning \n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "### Pandas - Paquete para manipulación de datos y análisis\n",
    "```bash\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado del texto \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento del texto\n",
    "### El preprocesamiento del texto consiste en curar los datos para la vectorización. Sin embargo, depende del dominio del problema.\n",
    "### Entre las actividades más comunes se tienen:\n",
    "- #### Remover acentos, números, símbolos duplicados  y símbolos raros\n",
    "- #### Convertir a minúsculas\n",
    "- #### Obtener unigramas, bigramas, trigramas de ***palabras***,  etc. \n",
    "- #### Obtener unigramas, bigramas, trigramas de ***caracteres***,  etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Lectura de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de archivos  TXT\n",
    "with open('./data/text_1.txt', 'r', encoding='utf-8') as archivo:\n",
    "    for linea in archivo:\n",
    "        print(linea.strip())  # Usamos strip() para quitar el salto de línea al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de archivos en formato JSON\n",
    "\n",
    "import pandas as pd\n",
    "d = pd.read_json(\"./data/text_3.json\", lines=True)\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceder a los datos del Frame\n",
    "\n",
    "d[\"text\"][300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertirlos en una lista\n",
    "\n",
    "list (d[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Normalización del texto\n",
    "- ### Eliminar acentos\n",
    "- ### Eliminar duplicados\n",
    "- ### Eliminar puntuación\n",
    "- ### mantiene solo un espacio entre los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza el texto \n",
    "# no hace la separación de tokens si están unidos por un símbolo o signo de puntuación. Realizar un proceso combinado, p. e., tokenización y normalizado \n",
    "\n",
    "texto = \"En Broadway, a la altura de la calle 113, no sólo se habla en un español nasal y contaminado;también podría decirse que se piensa, se camina y se come en español\"\n",
    "\n",
    "print(f\"Texto original: \\n\\t{texto}\\n\")\n",
    "texto_nuevo = normaliza_texto(texto)\n",
    "\n",
    "#Algunas palabras se juntan \"contaminadotambien\"\n",
    "\n",
    "print(texto_nuevo)\n",
    "\n",
    "texto_nuevo = normaliza_texto(texto, num=True)\n",
    "\n",
    "#Algunas palabras se juntan \"contaminadotambien\"\n",
    "\n",
    "print(texto_nuevo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio:\n",
    "- ### Normalizar el texto del archivo text_1.txt que se encuentra en la carpeta \"data\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO. Aplicar la normalización de texto a los datos del archivo \"text_1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Tokenización: Obtener las oraciones o  tokens del texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando datos de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalando los modelos, incluidos el español\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de las oraciones del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargar el tokenizador de oraciones para el español\n",
    "\n",
    "tokenizador_oraciones = nltk.data.load('tokenizers/punkt/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"De los cerros altos del sur, el de Luvina es el más alto y el más pedregoso. Está plagado de esa piedra gris con la que hacen la cal, pero en Luvina no hacen cal con ella\" \\\n",
    "        \" ni le sacan ningún provecho. Allí la llaman piedra cruda, y la loma que sube hacia Luvina la nombran Cuesta de la Piedra Cruda. El aire y el sol se han encargado de desmenuzarla,\" \\\n",
    "        \" de modo que la tierra de por allí es blanca y brillante como si estuviera rociada siempre por el rocío del amanecer; aunque esto es un puro decir, porque en Luvina los días son tan\" \\\n",
    "        \" fríos como las noches y el rocío se cuaja en el cielo antes que llegue a caer sobre la tierra.\"\n",
    "\n",
    "tokenizador_oraciones = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "oraciones = tokenizador_oraciones.tokenize(texto)\n",
    "print(oraciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar: Obtener los tokens del texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import word_tokenize\n",
    "\n",
    "texto = \"De los cerros altos del sur, el de Luvina es el más alto y el más pedregoso.\"\n",
    "\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Obtención de características para el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gramas o Unigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Un **unigrama** (puede ser unigrama de palabras o caracteres) es la unidad más básica dentro del análisis de texto en lingüística computacional y procesamiento del lenguaje natural (NLP). \n",
    "- #### Consiste en una única palabra o elemento dentro de una secuencia de texto. \n",
    "- #### Los unigramas se enfocan en el análisis palabra por palabra de forma individual.\n",
    "- #### Los unigramas suelen ser útiles en tareas como la clasificación de texto, el análisis de frecuencias de palabras y la identificación de temas clave dentro de un texto.\n",
    "\n",
    "\n",
    "Por ejemplo, si tomamos el texto:  \n",
    "\n",
    "**\"De los cerros altos del sur el de Luvina es el más alto\"**\n",
    "\n",
    "Los unigramas serían simplemente cada una de las palabras individuales:\n",
    "\n",
    "1. *De*\n",
    "2. *los*\n",
    "3. *cerros*\n",
    "4. *altos*\n",
    "5. *del*\n",
    "6. *sur*\n",
    "7. *el*\n",
    "8. *de*\n",
    "9. *Luvina*\n",
    "10. *es*\n",
    "11. *el*\n",
    "12. *más*\n",
    "13. *alto*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import word_tokenize\n",
    "\n",
    "texto = \"De los cerros altos del sur el de Luvina es el más alto\"\n",
    "\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: \n",
    "\n",
    "- ## Dado el texto \"texto1\":\n",
    "    -  ### Extraer las oraciones\n",
    "    -  ### Extraer las lista de tokens para cada oración\n",
    "\n",
    "```python\n",
    "\n",
    "texto1 = \"San Juan Luvina. Me sonaba a nombre de cielo aquel nombre. Pero aquello es el purgatorio. Un lugar moribundo donde se han muerto hasta los perros y ya no hay ni quien le ladre al silencio; pues en cuanto uno se acostumbra al vendaval que allí sopla, no se oye sino el silencio que hay en todas las soledades. Y eso acaba con uno. Míreme a mí. Conmigo acabó. Usted que va para allá comprenderá pronto lo que le digo.\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Procesar el texto1: separar las oraciones y extraer la lista  de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Procesar el texto1: separar las oraciones y extraer la lista  de tokens, normalizar el texto y  NO incluir los signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2-gramas o Bigramas\n",
    "#### Un bigrama es una secuencia de dos elementos contiguos dentro de un texto. Estos elementos pueden ser palabras, caracteres o unidades fonéticas, dependiendo del tipo de análisis que se realice. En el caso del análisis de texto, los bigramas más comunes son los formados por palabras consecutivas.\n",
    "\n",
    "#### Cada bigrama refleja la transición entre dos palabras dentro de la oración, y su análisis puede ser útil en áreas como la lingüística computacional, el análisis de texto o los modelos predictivos de lenguaje.\n",
    "\n",
    "#### Ejemplo:\n",
    "\n",
    "#### \"De los cerros altos del sur, el de Luvina es el más alto\"\n",
    "\n",
    "#### Al crear bigramas de palabras, tomamos dos palabras consecutivas a la vez. Los bigramas para esta frase serían:\n",
    "\n",
    "1. *De, los*\n",
    "2. *los, cerros*\n",
    "3. *cerros, altos*\n",
    "4. *altos, del*\n",
    "5. *del, sur*\n",
    "6. *sur, el*\n",
    "7. *el, de*\n",
    "8. *de, Luvina*\n",
    "9. *Luvina, es*\n",
    "10. *es, el*\n",
    "11. *el, más*\n",
    "12. *más, alto*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de bigramas\n",
    "from  nltk.util import bigrams\n",
    "\n",
    "texto = \"De los cerros altos del sur el de Luvina es el más alto\"\n",
    "tokens = word_tokenize(texto)\n",
    "bigramas = bigrams(tokens)\n",
    "bigramas = list(bigramas)\n",
    "bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3-gramas o Trigramas\n",
    "#### Un trigrama  es una secuencia de tres  elementos contiguos dentro de un texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de trigramas\n",
    "from  nltk.util import ngrams\n",
    "\n",
    "texto = \"De los cerros altos del sur el de Luvina es el más alto\"\n",
    "trigramas = ngrams(tokens, 3)\n",
    "trigramas = list(trigramas) \n",
    "trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: \n",
    "\n",
    "- ## Dado el archivo en formato JSON  \"text_3.json\" de la carpeta data:\n",
    "    -  ### Obtener los bigramas sin normalizar \n",
    "    -  ### Obtener los bigramas normalizados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Procesar el texto \"text_3.json\", separar las oraciones y extraer los bigramas sin normalizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:  Procesar el texto \"text_3.json\", separar las oraciones y extraer los bigramas normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1.4 Documentos representados como vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - En el **modelado de texto**, las palabras o frases se representan de forma **vectorial**, es decir, como vectores (listas de números). \n",
    "#### - Esto permite que los algoritmos de aprendizaje automático y procesamiento del lenguaje natural puedan trabajar con el texto, ya que estos modelos necesitan, principalmente, representaciones numéricas  para procesar datos. \n",
    "#### - Los métodos más comunes para convertir texto en vectores incluyen técnicas como **bag of words** (unigramas) presencia y ausencia, pesados **TF**, **TF-IDF**, entropía, y otros similares. \n",
    "#### - Otra representación popular son los **Word Embeddings** (como Word2Vec, GloVe, BERTs), que capturan tanto la presencia de palabras como su contexto semántico en un espacio numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# dos características por documento: (x1, x2) \n",
    "doc1=[5, 9]\n",
    "doc2=[7, 2]\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "x1 = [d[0] for d in docs]\n",
    "x2 = [d[1] for d in docs]\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.set_xlabel(\"x1\")\n",
    "axis.set_ylabel(\"x2\")\n",
    "axis.set_xlim(0, max(x1) + 1)\n",
    "axis.set_ylim(0, max(x2) + 1)\n",
    "axis.scatter(x1, x2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos representados como vectores\n",
    "- #### Se puede operar sobre los vectores. \n",
    "- #### Por ejemplo, calcular la similitud entre los Documentos A y B, por medio de la similitud coseno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ cosine(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\lVert \\vec{A} \\rVert \\lVert \\vec{B} \\rVert} =   \\frac{ \\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado del Copus de documentos\n",
    "- ### cada renglón representa un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"El cielo es azul\",\n",
    "\"El sol es brillante\",\n",
    "\"El sol en el cielo es brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"Juan Luis ama a maria\",\n",
    "\"Maria ama a Luis\",\n",
    "\"la estrella de la mañana y la estrella del atardecer es la misma estrella\",\n",
    "\"El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de vocabulario¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulario\n",
    "palabras=[]\n",
    "\n",
    "for d in docs:\n",
    "    for t in d.split():\n",
    "        palabras.append(t.lower())\n",
    "# Generación del vocabulario único\n",
    "vocabulario=list(sorted(set(palabras)))\n",
    "\n",
    "print(\"tokens:\",len(palabras))\n",
    "print(\"tokens únicos:\", len(vocabulario))\n",
    "vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Construcción de la matriz Documento-Término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construcción de la matriz Documento-Término (num_documentos x tamaño_vocabulario)\n",
    "# Pesado o ponderación de acuerdo a su frecuencia de aparición del término\n",
    "vsm =  np.zeros((len(docs), len(vocabulario)), dtype=float)\n",
    "for k, d in enumerate(docs):\n",
    "    for t in d.lower().split():\n",
    "        i = vocabulario.index(t)\n",
    "        vsm[k, i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forma de la matriz resultante\n",
    "vsm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los documentos en su representación vectorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Análisis de componentes principales\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "two_dim = pca.fit_transform(vsm)\n",
    "scatter_x = two_dim[:, 0] # primera componente principal\n",
    "scatter_y = two_dim[:, 1] # segunda componente principal\n",
    "plt.scatter(scatter_x, scatter_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del cálculo de la similitud entre dos documentos con la similitud coseno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ similitud\\_coseno(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\lVert \\vec{A} \\rVert \\lVert \\vec{B} \\rVert} =   \\frac{ \\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_texto(v):\n",
    "    # obtiene los índices de las palabras presentes en el vector\n",
    "    indices = np.flatnonzero(v>0)\n",
    "    for i in indices:\n",
    "        print(vocabulario[i], end=\" \")\n",
    "    print()\n",
    "\n",
    "def similitud_coseno(a, b):\n",
    "    return np.sum(a*b)/(np.sqrt(np.sum(np.power(a, 2))) * np.sqrt(np.sum(np.power(b, 2))))\n",
    "\n",
    "a = vsm[1]\n",
    "b = vsm[2]\n",
    "\n",
    "print(similitud_coseno(a,b))\n",
    "\n",
    "print_texto(a)\n",
    "print_texto(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Pesado de los términos según su importancia\n",
    "- #### TF: Term Frequency\n",
    "- #### TF-IDF: Term Frequency-Inverse Document Frecuency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesado TF: Term Frequency (Frecuencia del Término)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "#Ajusta el modelo con pesado TF, se obtiene la matriz Documento-Término\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "# Features o Componentes o Términos de la matriz \n",
    "print(vec.get_feature_names_out())\n",
    "\n",
    "#Tamaño del vocabulario\n",
    "print(\"vocabulario: \", len(vec.get_feature_names_out()))\n",
    "\n",
    "# Matriz  Documento-Término\n",
    "print(X.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizando la construcción de la matriz Documento-Término "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas y bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "# usar unigramas y bigramas como features:  ngram_range=(1,2)\n",
    "\n",
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "X = vec.fit_transform(docs)\n",
    "print(vec.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(\"vocabulario: \", len(vec.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar un preprocesamiento de datos personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\n",
    "\"El cielo es azul 5342534523534 !!!!!?????????!\",\n",
    "\"El sol es brillante  ?????!!!!!!\",\n",
    "\"El sol en el cielo es brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"JUAN LUIS,  ama MARíA !!!!!\",\n",
    "\"María ama a Luis\",\n",
    "\"la estrella de la mañana y la estrella del atardecer es la misma estrella\",\n",
    "\" El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    print(\"antes: \", texto)\n",
    "    texto = normaliza_texto(texto.lower())\n",
    "    print(\"después:\",texto)\n",
    "    return texto\n",
    "    \n",
    "vec = CountVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento,  ngram_range=(1,1))\n",
    "X = vec.fit_transform(docs)\n",
    "print(vec.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(\"vocabulario: \", len(vec.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pesado TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "- #### Es una técnica utilizada para evaluar la relevancia de una palabra dentro de un documento en relación con un conjunto de documentos (corpus). Combina dos conceptos:\n",
    "\n",
    "    - ##### TF (Frecuencia de Término): Mide cuántas veces aparece una palabra en un documento, ponderando la importancia de palabras más frecuentes.\n",
    "    - ##### IDF (Frecuencia Inversa de Documentos): Penaliza las palabras que aparecen en muchos documentos del corpus, dando más peso a las que son más raras o específicas.\n",
    "    - ##### El resultado es un valor que refleja la importancia de una palabra dentro de un documento, disminuyendo el peso de las palabras comunes y realzando las más distintivas. Es útil para tareas como clasificación de texto, búsqueda de información y extracción de características.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$TF-IDF(t,d) = TF(t,d) * IDF(t)$$\n",
    "$$TF(t,d) = f_{(t,d)}$$\n",
    "\n",
    "$$IDF(t)= \\log \\frac {N}{d_t + 1}  $$\n",
    "\n",
    "$$ N:  total\\ de\\ documentos $$ \n",
    "$$ d_t:  número\\ de\\ documentos \\ donde \\ aparece \\ el \\ término \\ t $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"El cielo es azul 5342534523534 !!!!!?????????!\",\n",
    "\"El sol es brillante  ?????!!!!!!\",\n",
    "\"El sol en el cielo es brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"JUAN LUIS,  ama MARíA !!!!!\",\n",
    "\"María ama a Luis\",\n",
    "\"la estrella de la mañana y la estrella del atardecer es la misma estrella\",\n",
    "\" El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    print(\"antes: \", texto)\n",
    "    texto = normaliza_texto(texto.lower())\n",
    "    print(\"después:\",texto)\n",
    "    return texto\n",
    "    \n",
    "    \n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(docs)\n",
    "print(vec_tfidf.get_feature_names_out())\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio:\n",
    " - ### Obtener la matriz Documento-Término del archivo text_3.json\n",
    " - ### Mejorar el preprocesamiento del texto: convertir a minúsculas, separar las oraciones, normalizar el texto, y separar los tokens adecuadamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    print(\"antes: \", texto)\n",
    "\n",
    "    #TODO: Mejorar el preprocesamiento, convertir a minúsculas, separar las oraciones, normalizar el texto, y separar los tokens\n",
    "\n",
    "\n",
    "\n",
    "    print(\"después:\",texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(docs)\n",
    "print(vec_tfidf.get_feature_names_out())\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Técnicas para reducir las características (**features**) textuales\n",
    "#### - **Stemming**.  Corta los finales de las palabras de acuerdo a un conjunto de reglas\n",
    "#### - **Lematización**. Reducir a la forma normal de la palabra, es decir, verbos a infinitivo; adjetivos a masculino, singular;  sustantivos a singular.\n",
    "#### - **Remover StopWords**. Eliminar palabras que no se consideran de contenido, es decir, que no aportan a la semántica como preposiciones: a, ante, de, desde, etc.; conjunciones entre otras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "### - El stemming es el proceso de reducir las palabras a su raíz o base sin considerar su contexto gramatical.\n",
    "### - A menudo, la raíz obtenida no es una palabra real, pero es útil para tareas de procesamiento de lenguaje natural.\n",
    "### - Algoritmos populares de stemming incluyen el **algoritmo de Porter** y el **algoritmo de Snowball**.\n",
    "### - Herramientas como **NLTK** incluyen implementaciones de estos algoritmos para varios idiomas.\n",
    "### - A diferencia de la lematización, el stemming es más rápido pero menos preciso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "print(stemmer.stem(\"mesas\"))\n",
    "print(stemmer.stem(\"mesa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematización \n",
    "\n",
    "### - La lematización es el proceso de reducir las palabras a su forma base o \"lema\", considerando su contexto gramatical.\n",
    "### - A diferencia del stemming, la lematización analiza la estructura lingüística para obtener una forma más precisa.\n",
    "### - Ejemplo: \"corriendo\" y \"corrió\" se reducen a \"correr\" (verbo en infinitivo).\n",
    "### - Herramientas como **spaCy** ofrecen lematización en varios idiomas.\n",
    "### - **NLTK** y **TextBlob** también permiten realizar lematización.\n",
    "### - **spaCy** es reconocido por su eficiencia y precisión al considerar el contexto gramatical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Instalación de los modelos de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "doc = nlp(\"me comí unas galletas con mis amigos, pero después Juan se comió la última que quedaba\")\n",
    "print(doc.text)\n",
    "print(\"=\"*20)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "- #### Las **stopwords** son palabras muy comunes en un idioma que suelen tener poco valor semántico o informativo en el análisis de texto. \n",
    "- #### En español, incluyen incluyen artículos, preposiciones y pronombres como  \"el\", \"la\", \"de\", \"y\", \"que\", \"es\", entre otros. \n",
    "- #### Estas palabras suelen eliminarse para reducir el ruido, se eliminan durante el preprocesamiento de texto para centrarse en términos más relevantes, facilitando tareas como la clasificación,  análisis de sentimientos, búsqueda y minería de texto.\n",
    "- #### Herramientas como **NLTK**, **spaCy** y **scikit-learn** ofrecen listas predeterminadas de stopwords.\n",
    "- #### Las stopwords se adaptan al idioma y a la tarea, por lo que a veces es útil personalizar la lista según el contexto del problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga de las listas de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "print(_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"El cielo es azul,  5342534523534 !!!!!?????????!\",\n",
    "\"El sol es,brillante  ?????!!!!!!\",\n",
    "\"El sol en el, cielo,es muy  brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"JUAN LUIS,  ama MARíA !!!!!\",\n",
    "\"María ama a Luis\",\n",
    "\"la estrella de la mañana y la estrella del atardecer es la misma estrella\",\n",
    "\" El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor semántico al texto\n",
    "    print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "\n",
    "    print(\"después:\",texto)\n",
    "    \n",
    "    return texto\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(docs)\n",
    "print(vec_tfidf.get_feature_names_out())\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir los ejemplos de prueba al mismo espacio de representación de datos del conjunto de entrenamiento que aprendió el modelo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = [\n",
    "\"El sol es muy brillante el día de hoy\",\n",
    "\"Podemos ver el cielo azul\",\n",
    "\"JUAN LUIS   se fue de paseo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El método transform convierte la entrada al espacio de representación del texto modelado con los datos de entrenamiento\n",
    "doc_test_tfidf = vec_tfidf.transform(doc_test)\n",
    "print(doc_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspección de los datos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tamaño de la matriz de prueba: (ejemplos, tamaño de features) \", doc_test_tfidf.shape)\n",
    "n_sample=0\n",
    "print(\"\\ntexto original: \")\n",
    "print(doc_test[n_sample])\n",
    "#type(vec_tfidf.inverse_transform(X_test_tfidf[n_sample]))\n",
    "\n",
    "print(\"\\ntexto del espacio de representación: \")\n",
    "print(\" \".join(vec_tfidf.inverse_transform(doc_test_tfidf[n_sample])[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Preparación de los conjuntos de datos para el entrenamiento y evaluación</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Cargar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Lee el archivo de datos: el formato es una linea es un ejemplo de entrenamiento.\n",
    "# dataset = pd.read_json(\"./data/data_emotions_es.json\", lines=True)\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "\n",
    "\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "\n",
    "#primeros 30 textos\n",
    "print(dataset[\"text\"].to_list()[:30])\n",
    "\n",
    "print(dataset.klass.value_counts())\n",
    "\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "\n",
    "Y = dataset['klass'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificar las categorías utilizando un esquema de codificación ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.classes_)\n",
    "\n",
    "n=5\n",
    "print(\"Clases codificadas:\")\n",
    "print(Y[:n], Y_encoded[:n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decodificar las categorías en  codificación ordinal a la categoría original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arreglo con  resultados de las clases en la categoría codificada. \n",
    "le.inverse_transform([0, 0 , 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preparación de los conjuntos de datos  (datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hay  dos formas principales de evaluación de modelos: partición y validación cruzada:\n",
    "\n",
    "### 1. **Evaluación por partición (separación de datasets en train-test)**:\n",
    "   - #### Dividir el conjunto de datos en dos subconjuntos: uno para entrenar (train) y otro para probar (test).\n",
    "   - #### Proceso simple y rápido, adecuado para conjuntos de datos grandes.\n",
    "   - #### Riesgo de que la evaluación dependa de cómo se haya dividido el conjunto, lo que puede generar un rendimiento sesgado.\n",
    "   - #### Se utiliza un porcentaje fijo de datos para entrenar y el resto para evaluar, como 70%/30% o 80%/20%.\n",
    "   \n",
    "### 2. **Evaluación por validación cruzada**:\n",
    "   - #### El conjunto de datos se divide en múltiples subconjuntos (folds) y se entrena el modelo varias veces, usando cada fold como conjunto de prueba y el resto como entrenamiento.\n",
    "   - #### Más robusta y fiable, ya que la evaluación se basa en múltiples particiones, lo que reduce el sesgo.\n",
    "   - #### Ayuda a obtener una estimación más precisa del rendimiento del modelo en datos no vistos.\n",
    "   - #### La variante más común es la **k-fold cross-validation**, donde los datos se dividen en *k* subconjuntos.\n",
    "\n",
    "\n",
    "      <img src=\"figs/fig_cross-validation.jpg\" width=\"800\">\n",
    "\n",
    "\n",
    "###### Fuente: https://es.wikipedia.org/wiki/Validación_cruzada\n",
    "\n",
    "### **Ambos métodos tienen ventajas y desventajas, siendo la validación cruzada más precisa pero costosa en términos computacionales.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de datos por partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas: test_size = 0.2 (20%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify= Y_encoded, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostrar los primeros N ejemplos con sus clases\n",
    "samples=10\n",
    "X_train[:samples], Y_train[:samples], le.inverse_transform(Y_train[:samples]), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separación de datos para validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#tamaño de los k-folds\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)  \n",
    "for k, (index_train, index_test) in enumerate(skf.split(X, Y_encoded), start=1):\n",
    "    \n",
    "    # index_train y index_test obtienen los índices de las muestras para procesar\n",
    "    X_train_cv, X_test_cv = X[index_train], X[index_test]\n",
    "\n",
    "    Y_train_cv, Y_test_cv = Y_encoded[index_train], Y_encoded[index_test]\n",
    "\n",
    "    print(\"k = {}\".format(k))\n",
    "    print('train [num. clases]-  {}   |   test [num. clases]-  {}'.format(\n",
    "        np.bincount(Y_train_cv), np.bincount(Y_test_cv) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Ejercicio: </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Preprocesar los datos para la creación de la matriz de documento-término, usar un pesado TF-IDF, y unigramas\n",
    "\n",
    "###  2. Preparar los conjuntos de datos para el entrenamiento con un partición train-test: dividir 70% para entrenamiento y 30% para test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cargar el conjunto de datos \"data_aggressiveness_es.json\" \n",
    "\n",
    "\n",
    "# TODO: Preprocesar los datos: convertir a minúsculas, borrar puntuación, etc.\n",
    "\n",
    "\n",
    "# TODO: Codificar las clases si no son categorías ordinales.\n",
    "\n",
    "\n",
    "# TODO: Separar los datos 70% entrenamiento y 30% test\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Contruir la matriz de documento-término  para el conjunto de entrenamiento y para el conjunto de test \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
