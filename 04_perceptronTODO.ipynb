{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica. \n",
    "\n",
    "### Construir la siguiente red de perceptrones para identificar números de una matriz de 5x3 pixeles representados por 0s y 1s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/fig-num_perceptron.png\" width=\"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de entrada: X\n",
    "####  Cada fila representa un número entre 0 y 9 codificado en bits en una 'matriz' de 5x3\n",
    "\n",
    "### Datos de Salida: T\n",
    "####  Cada fila representa un número entre 0 y 9 codificado en forma binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de entrada (cada fila representa un número en bits)\n",
    "X = np.array([\n",
    "    [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1],\n",
    "    [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# Salida deseada de la red neuronal: números codificados en binario\n",
    "T = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los datos para la comprensión del dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convertir la matriz numpy a una imagen\n",
    "# Visualización del  numero N\n",
    "N=9\n",
    "numero = X[N]   \n",
    "print(numero)\n",
    "bit_image = np.array( numero, dtype=np.uint8).reshape(5,3)  \n",
    "bit_image = bit_image  * 143 # Multiplicamos por 143 para convertir a escala de grises\n",
    "print(bit_image)\n",
    "plt.imshow(bit_image, cmap='gray',  vmin=0, vmax=255)\n",
    "plt.axis('off')  # Ocultar los ejes\n",
    "# plt.savefig(\"test.png\", bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función de activación será la función escalón $ f(x) $, se define como sigue:\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "1 & \\text{si } x \\geq 0 \\\\\n",
    "0 & \\text{si } x < 0 \n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones:\n",
    "### - Implementar la red de perceptrones como se muestra en la figura anterior. \n",
    "### - En la clase `PerceptronTODO`, se proporciona un esquema general de la red de perceptrones.\n",
    "### - Completar lo indicado en la clase para construir una clasificador usando la red de perceptrones mencionado.\n",
    "### - Aplicar el procedimiento para el entrenamiento del clasificador\n",
    "### - Haga uso de la regla de aprendizaje del perceptrón para actualización de los pesos.\n",
    "\n",
    "$$\n",
    "\\Large \\Delta w_i \\leftarrow  \\eta \\cdot (t - y ) \\cdot x_i \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\Large w_i \\leftarrow w_i + \\Delta w_i \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Large b \\leftarrow b + \\eta \\cdot (t - y)\n",
    "$$\n",
    "\n",
    "### Donde:\n",
    "- ### $\\eta$ representa la tasa de aprendizaje ($learing rate$), en este caso, se usará la constante con valor de 1.\n",
    "- ### $\\Delta w_i$ representa el cambio necesario para ajustar el parámetro $w_i$\n",
    "- ### $b$ representa el sesgo o $bias$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PerceptronTODO:\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=1, epochs=1000):\n",
    "        # Inicializa las dimensiones de entrada y salida, la tasa de aprendizaje y el número de épocas\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        # TODO: Inicializar la matriz de pesos (con ceros o con valores aleatorios, probar ambos enfoques)\n",
    "        \n",
    "        # self.weights = ?\n",
    "\n",
    "        # TODO: Inicializar el sesgo (bias) (con ceros o con valores aleatorios, probar ambos enfoques)\n",
    "        \n",
    "        #self.bias = ?\n",
    "\n",
    "    def step_function(self, x):\n",
    "        # TODO: Implementa la función escalón \n",
    "        # return ?\n",
    "        return \n",
    "\n",
    "    def train(self, X, T):\n",
    "\n",
    "        # Etapa de entrenamiento.\n",
    "        # X: contiene todo el conjunto de entrenamiento.\n",
    "        # T: contienen todas las salidas esperadas.\n",
    "\n",
    "        # Una época indica que pasó por todos los ejemplos de entrenamiento \n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            print(\"Epoca: \",  _)\n",
    "            # TODO: Propagación hacia adelante, calcula la entrada neta (z) de las neuronas\n",
    "\n",
    "            # z = ??????\n",
    "\n",
    "\n",
    "            # TODO: Calcula la salida aplicando la función escalón como función de activación\n",
    "\n",
    "            #  y = ?\n",
    "            \n",
    "            # TODO: Calcula el error como la diferencia entre la salida deseada T y la salida actual\n",
    "               # err = \n",
    "\n",
    "            # TODO: Actualiza los pesos usando la regla de aprendizaje del perceptrón\n",
    "\n",
    "            \n",
    "\n",
    "            # TODO: Actualiza el sesgo usando la regla de aprendizaje del perceptrón\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        # Etap\n",
    "        # TODO: Calcula la entrada neta de las neuronas (z) y devuelve la predicción aplicando la función de activación\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de uso para el entrenamiento  de la clase PerceptronTODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el perceptrón\n",
    "#Dimensines de entrada\n",
    "dim_entrada = X.shape[1]\n",
    "dim_salida = T.shape[1]\n",
    "perceptron = PerceptronTODO(input_dim=dim_entrada, output_dim=dim_salida, learning_rate=1, epochs=1000)\n",
    "\n",
    "# Entrenar el modelo\n",
    "# Encuentra los valores óptimos para los parámetros (pesos y bias):  W y B\n",
    "perceptron.train(X, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de la etapa de predicción de nuevos datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones\n",
    "n8 = np.array([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]) #8\n",
    "\n",
    "predictions = perceptron.predict(n8)\n",
    "\n",
    "# Imprimir las predicciones\n",
    "print(\"Predicciones:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio extra: Graficar el error de la etapa de entrenamiento.\n",
    "\n",
    "### - El eje x indicaría la época.\n",
    "\n",
    "### - El eje y indicaría el error.\n",
    "\n",
    "### - El error se puede calcular por medio del Error Cuadrático Medio (MSE) o el Error Absoluto Medio (MAE). Básicamente se calculan los errores de todos los ejemplos de entrenamiento para una época y se promedian. Estos se acumulan en un error general para graficarlos posteriormente al finalizar el entrenamiento. \n",
    "\n",
    "### - Graficar las dos medidas en gráficos independientes.\n",
    "\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "## 1. Error Cuadrático Medio (MSE - *Mean Squared Error*)\n",
    "\n",
    "\n",
    "\n",
    "### El Error Cuadrático Medio mide el promedio de los errores al cuadrado entre las predicciones del modelo y los valores reales. La fórmula es:\n",
    "\n",
    "$$\n",
    "\\Large \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (t_i - y_i)^2\n",
    "$$\n",
    "\n",
    "### Donde:\n",
    "- ### $ n $ es el número total de muestras o ejemplos de entrenamiento.\n",
    "- ### $ t_i $ es el valor real para la muestra $ i $.\n",
    "- ### $ y_i $ es el valor predicho para la muestra $ i $.\n",
    "\n",
    "</br>\n",
    "\n",
    "## Para múltiples salidas:\n",
    "\n",
    "$$\n",
    "\\Large \\text{MSE} = \\frac{1}{n \\cdot m} \\sum_{i=1}^{n} \\sum_{j=1}^{m} (t_{ij} - {y}_{ij})^2\n",
    "$$\n",
    "\n",
    "### donde:\n",
    "- ### $n$: número de muestras o patrones\n",
    "- ### $m$: número de salidas\n",
    "- ### $t_{ij}$: valor real de la salida $j$ para la muestra $i$\n",
    "- ### ${y}_{ij}$: valor predicho de la salida $j$ para la muestra $i$\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "## 2. Error Absoluto Medio (MAE- *Mean Absolute Error* )\n",
    "\n",
    "###  El Error Absoluto Medio mide el promedio de los errores absolutos entre las predicciones del modelo y los valores reales. Es menos sensible a los valores atípicos en comparación con el MSE. La fórmula es:\n",
    "\n",
    "$$\n",
    "\\Large \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |t_i - y_i|\n",
    "$$\n",
    "\n",
    "### Donde:\n",
    "- ### $ n $ es el número total de muestras o ejemplos de entrenamiento.\n",
    "- ### $ t_i $ es el valor real para la muestra $ i $.\n",
    "- ### $ y_i $ es el valor predicho para la muestra $ i $.\n",
    "- ### $|\\cdot|$: valor absoluto\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "## Para múltiples salidas:\n",
    "\n",
    "$$\n",
    "\\Large \\text{MAE} = \\frac{1}{n \\cdot m} \\sum_{i=1}^{n} \\sum_{j=1}^{m} |t_{ij} - {y}_{ij}|\n",
    "$$\n",
    "\n",
    "### donde:\n",
    "- ### $n$: número de muestras o patrones\n",
    "- ### $m$: número de salidas\n",
    "- ### $t_{ij}$: valor real de la salida $j$ para la muestra $i$\n",
    "- ### ${y}_{ij}$: valor predicho de la salida $j$ para la muestra $i$\n",
    "- ### $|\\cdot|$: valor absoluto\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "## Diferencias Clave:\n",
    "\n",
    "- ### **MSE** penaliza más los errores grandes debido a la elevación al cuadrado de las diferencias. Esto puede hacer que el modelo se enfoque en minimizar errores grandes, pero puede ser más sensible a los valores atípicos.\n",
    "\n",
    "- ### **MAE** da una medida más robusta de los errores, ya que no eleva al cuadrado las diferencias, por lo que es menos sensible a los valores atípicos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
