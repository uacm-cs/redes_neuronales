{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esquema General de un clasificador\n",
    "\n",
    "\n",
    "<img src=\"figs/fig-diagrama-clasificador.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos para construir el clasificador\n",
    "\n",
    "## 1. Modelado del texto\n",
    "- ### 1.1 Preprocesamiento del texto: incluye remover signos, stopwords, n√∫meros, es decir, datos que no aporten a la sem√°ntica del texto\n",
    "- ### 1.2 Vectorizar el texto: convertir el texto a una forma que la computadora pueda procesar\n",
    "    - ### 1.2.1 Calcular la importancia de las componentes (t√©rminos) de cada vector: \n",
    "        - #### Pesado de t√©rminos como Frecuencia de T√©rmino (TF) \n",
    "        - #### Frecuencia de T√©rmino ‚Äì Frecuencia Inversa de Documento (TF-IDF)\n",
    "\n",
    "    - ### 1.3 Tambi√©n se puede usar una representaci√≥n basada en Word Embeddings\n",
    "        - #### Glove\n",
    "        - #### Word2Vec\n",
    "        - #### BERTs\n",
    "\n",
    "##  2. Entrenar un clasificador para construir el modelo con los datos procesados\n",
    "###  Usar un clasificador como:\n",
    "- ###  Red Neuronal\n",
    "- ###  Support Vector Machine\n",
    "- ###  K-NN (K vecinos m√°s cercanos)\n",
    "- ###  etc.\n",
    "\n",
    "##  3. Evaluar el desempe√±o del clasificador construido\n",
    "###  Usar m√©tricas como:\n",
    "- ###  Precisi√≥n\n",
    "- ###  Recall\n",
    "- ###  F1-score\n",
    "\n",
    "##  4. Desplegar el modelo para su uso en un ambiente de pruebas y eventualmente en producci√≥n\n",
    "\n",
    "###  Por ejemplo:\n",
    "- ###  Aplicaci√≥n WEB\n",
    "- ###  API REST\n",
    "- ###  API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalaci√≥n de paquetes para Procesamiento de Lenguaje Natural\n",
    "\n",
    "\n",
    "### NLTK - Paquete para procesamiento de lenguaje natural: Natural Language Tool Kit\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "### spaCY - Paquete para procesamiento de lenguaje natural\n",
    "```bash\n",
    "pip install setuptools wheel\n",
    "pip install spacy==3.7.6\n",
    "```\n",
    "\n",
    "### scikit-learn - Paquete de Machine Learning \n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "### Pandas - Paquete para manipulaci√≥n de datos y an√°lisis\n",
    "```bash\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado del texto \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento del texto\n",
    "### El preprocesamiento del texto consiste en curar los datos para la vectorizaci√≥n. Sin embargo, depende del dominio del problema.\n",
    "### Entre las actividades m√°s comunes se tienen:\n",
    "- #### Remover acentos, n√∫meros, s√≠mbolos duplicados  y s√≠mbolos raros\n",
    "- #### Convertir a min√∫sculas\n",
    "- #### Obtener unigramas, bigramas, trigramas de ***palabras***,  etc. \n",
    "- #### Obtener unigramas, bigramas, trigramas de ***caracteres***,  etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Lectura de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de archivos  TXT\n",
    "with open('./data/text_1.txt', 'r', encoding='utf-8') as archivo:\n",
    "    for linea in archivo:\n",
    "        print(linea.strip())  # Usamos strip() para quitar el salto de l√≠nea al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de archivos en formato JSON\n",
    "\n",
    "import pandas as pd\n",
    "d = pd.read_json(\"./data/text_3.json\", lines=True)\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceder a los datos del Frame\n",
    "\n",
    "d[\"text\"][300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertirlos en una lista\n",
    "\n",
    "list (d[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Normalizaci√≥n del texto\n",
    "- ### Eliminar acentos\n",
    "- ### Eliminar duplicados\n",
    "- ### Eliminar puntuaci√≥n\n",
    "- ### mantiene solo un espacio entre los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¬ø?¬°!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuaci√≥n, True deja intacta la puntuaci√≥n)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los n√∫meros, True deja intactos los acentos)\n",
    "        max_dup=2 (n√∫mero m√°ximo de s√≠mbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza el texto \n",
    "# no hace la separaci√≥n de tokens si est√°n unidos por un s√≠mbolo o signo de puntuaci√≥n. Realizar un proceso combinado, p. e., tokenizaci√≥n y normalizado \n",
    "\n",
    "texto = \"En Broadway, a la altura de la calle 113, no soÃÅlo se habla en un espanÃÉol nasal y contaminado;tambieÃÅn podriÃÅa decirse que se piensa, se camina y se come en espanÃÉol\"\n",
    "\n",
    "print(f\"Texto original: \\n\\t{texto}\\n\")\n",
    "texto_nuevo = normaliza_texto(texto)\n",
    "\n",
    "#Algunas palabras se juntan \"contaminadotambien\"\n",
    "\n",
    "print(texto_nuevo)\n",
    "\n",
    "texto_nuevo = normaliza_texto(texto, num=True)\n",
    "\n",
    "#Algunas palabras se juntan \"contaminadotambien\"\n",
    "\n",
    "print(texto_nuevo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio:\n",
    "- ### Normalizar el texto del archivo text_1.txt que se encuentra en la carpeta \"data\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO. Aplicar la normalizaci√≥n de texto a los datos del archivo \"text_1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Tokenizaci√≥n: Obtener las oraciones o  tokens del texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando datos de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalando los modelos, incluidos el espa√±ol\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separaci√≥n de las oraciones del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargar el tokenizador de oraciones para el espa√±ol\n",
    "\n",
    "tokenizador_oraciones = nltk.data.load('tokenizers/punkt/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"De los cerros altos del sur, el de Luvina es el m√°s alto y el m√°s pedregoso. Est√° plagado de esa piedra gris con la que hacen la cal, pero en Luvina no hacen cal con ella\" \\\n",
    "        \" ni le sacan ning√∫n provecho. All√≠ la llaman piedra cruda, y la loma que sube hacia Luvina la nombran Cuesta de la Piedra Cruda. El aire y el sol se han encargado de desmenuzarla,\" \\\n",
    "        \" de modo que la tierra de por all√≠ es blanca y brillante como si estuviera rociada siempre por el roc√≠o del amanecer; aunque esto es un puro decir, porque en Luvina los d√≠as son tan\" \\\n",
    "        \" fr√≠os como las noches y el roc√≠o se cuaja en el cielo antes que llegue a caer sobre la tierra.\"\n",
    "\n",
    "tokenizador_oraciones = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "oraciones = tokenizador_oraciones.tokenize(texto)\n",
    "print(oraciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar: Obtener los tokens del texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import word_tokenize\n",
    "\n",
    "texto = \"De los cerros altos del sur, el de Luvina es el m√°s alto y el m√°s pedregoso.\"\n",
    "\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Obtenci√≥n de caracter√≠sticas para el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gramas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gramas o Unigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Un **unigrama** (puede ser unigrama de palabras o caracteres) es la unidad m√°s b√°sica dentro del an√°lisis de texto en ling√º√≠stica computacional y procesamiento del lenguaje natural (NLP). \n",
    "- #### Consiste en una √∫nica palabra o elemento dentro de una secuencia de texto. \n",
    "- #### Los unigramas se enfocan en el an√°lisis palabra por palabra de forma individual.\n",
    "- #### Los unigramas suelen ser √∫tiles en tareas como la clasificaci√≥n de texto, el an√°lisis de frecuencias de palabras y la identificaci√≥n de temas clave dentro de un texto.\n",
    "\n",
    "\n",
    "Por ejemplo, si tomamos el texto:  \n",
    "\n",
    "**\"De los cerros altos del sur el de Luvina es el m√°s alto\"**\n",
    "\n",
    "Los unigramas ser√≠an simplemente cada una de las palabras individuales:\n",
    "\n",
    "1. *De*\n",
    "2. *los*\n",
    "3. *cerros*\n",
    "4. *altos*\n",
    "5. *del*\n",
    "6. *sur*\n",
    "7. *el*\n",
    "8. *de*\n",
    "9. *Luvina*\n",
    "10. *es*\n",
    "11. *el*\n",
    "12. *m√°s*\n",
    "13. *alto*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import word_tokenize\n",
    "\n",
    "texto = \"De los cerros altos del sur el de Luvina es el m√°s alto\"\n",
    "\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: \n",
    "\n",
    "- ## Dado el texto \"texto1\":\n",
    "    -  ### Extraer las oraciones\n",
    "    -  ### Extraer las lista de tokens para cada oraci√≥n\n",
    "\n",
    "```python\n",
    "\n",
    "texto1 = \"San Juan Luvina. Me sonaba a nombre de cielo aquel nombre. Pero aquello es el purgatorio. Un lugar moribundo donde se han muerto hasta los perros y ya no hay ni quien le ladre al silencio; pues en cuanto uno se acostumbra al vendaval que all√≠ sopla, no se oye sino el silencio que hay en todas las soledades. Y eso acaba con uno. M√≠reme a m√≠. Conmigo acab√≥. Usted que va para all√° comprender√° pronto lo que le digo.\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Procesar el texto1: separar las oraciones y extraer la lista  de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Procesar el texto1: separar las oraciones y extraer la lista  de tokens, normalizar el texto y  NO incluir los signos de puntuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2-gramas o Bigramas\n",
    "#### Un bigrama es una secuencia de dos elementos contiguos dentro de un texto. Estos elementos pueden ser palabras, caracteres o unidades fon√©ticas, dependiendo del tipo de an√°lisis que se realice. En el caso del an√°lisis de texto, los bigramas m√°s comunes son los formados por palabras consecutivas.\n",
    "\n",
    "#### Cada bigrama refleja la transici√≥n entre dos palabras dentro de la oraci√≥n, y su an√°lisis puede ser √∫til en √°reas como la ling√º√≠stica computacional, el an√°lisis de texto o los modelos predictivos de lenguaje.\n",
    "\n",
    "#### Ejemplo:\n",
    "\n",
    "#### \"De los cerros altos del sur, el de Luvina es el m√°s alto\"\n",
    "\n",
    "#### Al crear bigramas de palabras, tomamos dos palabras consecutivas a la vez. Los bigramas para esta frase ser√≠an:\n",
    "\n",
    "1. *De, los*\n",
    "2. *los, cerros*\n",
    "3. *cerros, altos*\n",
    "4. *altos, del*\n",
    "5. *del, sur*\n",
    "6. *sur, el*\n",
    "7. *el, de*\n",
    "8. *de, Luvina*\n",
    "9. *Luvina, es*\n",
    "10. *es, el*\n",
    "11. *el, m√°s*\n",
    "12. *m√°s, alto*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo de bigramas\n",
    "from  nltk.util import bigrams\n",
    "\n",
    "texto = \"De los cerros altos del sur el de Luvina es el m√°s alto\"\n",
    "tokens = word_tokenize(texto)\n",
    "bigramas = bigrams(tokens)\n",
    "bigramas = list(bigramas)\n",
    "bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3-gramas o Trigramas\n",
    "#### Un trigrama  es una secuencia de tres  elementos contiguos dentro de un texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo de trigramas\n",
    "from  nltk.util import ngrams\n",
    "\n",
    "texto = \"De los cerros altos del sur el de Luvina es el m√°s alto\"\n",
    "trigramas = ngrams(tokens, 3)\n",
    "trigramas = list(trigramas) \n",
    "trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: \n",
    "\n",
    "- ## Dado el archivo en formato JSON  \"text_3.json\" de la carpeta data:\n",
    "    -  ### Obtener los bigramas sin normalizar \n",
    "    -  ### Obtener los bigramas normalizados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Procesar el texto \"text_3.json\", separar las oraciones y extraer los bigramas sin normalizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:  Procesar el texto \"text_3.json\", separar las oraciones y extraer los bigramas normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1.4 Documentos representados como vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - En el **modelado de texto**, las palabras o frases se representan de forma **vectorial**, es decir, como vectores (listas de n√∫meros). \n",
    "#### - Esto permite que los algoritmos de aprendizaje autom√°tico y procesamiento del lenguaje natural puedan trabajar con el texto, ya que estos modelos necesitan, principalmente, representaciones num√©ricas  para procesar datos. \n",
    "#### - Los m√©todos m√°s comunes para convertir texto en vectores incluyen t√©cnicas como **bag of words** (unigramas) presencia y ausencia, pesados **TF**, **TF-IDF**, entrop√≠a, y otros similares. \n",
    "#### - Otra representaci√≥n popular son los **Word Embeddings** (como Word2Vec, GloVe, BERTs), que capturan tanto la presencia de palabras como su contexto sem√°ntico en un espacio num√©rico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# dos caracter√≠sticas por documento: (x1, x2) \n",
    "doc1=[5, 9]\n",
    "doc2=[7, 2]\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "x1 = [d[0] for d in docs]\n",
    "x2 = [d[1] for d in docs]\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.set_xlabel(\"x1\")\n",
    "axis.set_ylabel(\"x2\")\n",
    "axis.set_xlim(0, max(x1) + 1)\n",
    "axis.set_ylim(0, max(x2) + 1)\n",
    "axis.scatter(x1, x2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentos representados como vectores\n",
    "- #### Se puede operar sobre los vectores. \n",
    "- #### Por ejemplo, calcular la similitud entre los Documentos A y B, por medio de la similitud coseno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ cosine(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\lVert \\vec{A} \\rVert \\lVert \\vec{B} \\rVert} =   \\frac{ \\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado del Copus de documentos\n",
    "- ### cada rengl√≥n representa un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"El cielo es azul\",\n",
    "\"El sol es brillante\",\n",
    "\"El sol en el cielo es brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"Juan Luis ama a maria\",\n",
    "\"Maria ama a Luis\",\n",
    "\"la estrella de la ma√±ana y la estrella del atardecer es la misma estrella\",\n",
    "\"El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n de vocabulario¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulario\n",
    "palabras=[]\n",
    "\n",
    "for d in docs:\n",
    "    for t in d.split():\n",
    "        palabras.append(t.lower())\n",
    "# Generaci√≥n del vocabulario √∫nico\n",
    "vocabulario=list(sorted(set(palabras)))\n",
    "\n",
    "print(\"tokens:\",len(palabras))\n",
    "print(\"tokens √∫nicos:\", len(vocabulario))\n",
    "vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Construcci√≥n de la matriz Documento-T√©rmino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construcci√≥n de la matriz Documento-T√©rmino (num_documentos x tama√±o_vocabulario)\n",
    "# Pesado o ponderaci√≥n de acuerdo a su frecuencia de aparici√≥n del t√©rmino\n",
    "vsm =  np.zeros((len(docs), len(vocabulario)), dtype=float)\n",
    "for k, d in enumerate(docs):\n",
    "    for t in d.lower().split():\n",
    "        i = vocabulario.index(t)\n",
    "        vsm[k, i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forma de la matriz resultante\n",
    "vsm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n de los documentos en su representaci√≥n vectorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# An√°lisis de componentes principales\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "two_dim = pca.fit_transform(vsm)\n",
    "scatter_x = two_dim[:, 0] # primera componente principal\n",
    "scatter_y = two_dim[:, 1] # segunda componente principal\n",
    "plt.scatter(scatter_x, scatter_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del c√°lculo de la similitud entre dos documentos con la similitud coseno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ similitud\\_coseno(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\lVert \\vec{A} \\rVert \\lVert \\vec{B} \\rVert} =   \\frac{ \\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_texto(v):\n",
    "    # obtiene los √≠ndices de las palabras presentes en el vector\n",
    "    indices = np.flatnonzero(v>0)\n",
    "    for i in indices:\n",
    "        print(vocabulario[i], end=\" \")\n",
    "    print()\n",
    "\n",
    "def similitud_coseno(a, b):\n",
    "    return np.sum(a*b)/(np.sqrt(np.sum(np.power(a, 2))) * np.sqrt(np.sum(np.power(b, 2))))\n",
    "\n",
    "a = vsm[1]\n",
    "b = vsm[2]\n",
    "\n",
    "print(similitud_coseno(a,b))\n",
    "\n",
    "print_texto(a)\n",
    "print_texto(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Pesado de los t√©rminos seg√∫n su importancia\n",
    "- #### TF: Term Frequency\n",
    "- #### TF-IDF: Term Frequency-Inverse Document Frecuency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesado TF: Term Frequency (Frecuencia del T√©rmino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "#Ajusta el modelo con pesado TF, se obtiene la matriz Documento-T√©rmino\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "# Features o Componentes o T√©rminos de la matriz \n",
    "print(vec.get_feature_names_out())\n",
    "\n",
    "#Tama√±o del vocabulario\n",
    "print(\"vocabulario: \", len(vec.get_feature_names_out()))\n",
    "\n",
    "# Matriz  Documento-T√©rmino\n",
    "print(X.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizando la construcci√≥n de la matriz Documento-T√©rmino "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramas y bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "# usar unigramas y bigramas como features:  ngram_range=(1,2)\n",
    "\n",
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "X = vec.fit_transform(docs)\n",
    "print(vec.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(\"vocabulario: \", len(vec.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar un preprocesamiento de datos personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\n",
    "\"El cielo es azul 5342534523534 !!!!!?????????!\",\n",
    "\"El sol es brillante  ?????!!!!!!\",\n",
    "\"El sol en el cielo es brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"JUAN LUIS,  ama MAR√≠A !!!!!\",\n",
    "\"Mar√≠a ama a Luis\",\n",
    "\"la estrella de la ma√±ana y la estrella del atardecer es la misma estrella\",\n",
    "\" El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a min√∫sculas el texto antes de normalizar\n",
    "    print(\"antes: \", texto)\n",
    "    texto = normaliza_texto(texto.lower())\n",
    "    print(\"despu√©s:\",texto)\n",
    "    return texto\n",
    "    \n",
    "vec = CountVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento,  ngram_range=(1,1))\n",
    "X = vec.fit_transform(docs)\n",
    "print(vec.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "print(\"vocabulario: \", len(vec.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pesado TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "- #### Es una t√©cnica utilizada para evaluar la relevancia de una palabra dentro de un documento en relaci√≥n con un conjunto de documentos (corpus). Combina dos conceptos:\n",
    "\n",
    "    - ##### TF (Frecuencia de T√©rmino): Mide cu√°ntas veces aparece una palabra en un documento, ponderando la importancia de palabras m√°s frecuentes.\n",
    "    - ##### IDF (Frecuencia Inversa de Documentos): Penaliza las palabras que aparecen en muchos documentos del corpus, dando m√°s peso a las que son m√°s raras o espec√≠ficas.\n",
    "    - ##### El resultado es un valor que refleja la importancia de una palabra dentro de un documento, disminuyendo el peso de las palabras comunes y realzando las m√°s distintivas. Es √∫til para tareas como clasificaci√≥n de texto, b√∫squeda de informaci√≥n y extracci√≥n de caracter√≠sticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$TF-IDF(t,d) = TF(t,d) * IDF(t)$$\n",
    "$$TF(t,d) = f_{(t,d)}$$\n",
    "\n",
    "$$IDF(t)= \\log \\frac {N}{d_t + 1}  $$\n",
    "\n",
    "$$ N:  total\\ de\\ documentos $$ \n",
    "$$ d_t:  n√∫mero\\ de\\ documentos \\ donde \\ aparece \\ el \\ t√©rmino \\ t $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"El cielo es azul 5342534523534 !!!!!?????????!\",\n",
    "\"El sol es brillante  ?????!!!!!!\",\n",
    "\"El sol en el cielo es brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"JUAN LUIS,  ama MAR√≠A !!!!!\",\n",
    "\"Mar√≠a ama a Luis\",\n",
    "\"la estrella de la ma√±ana y la estrella del atardecer es la misma estrella\",\n",
    "\" El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a min√∫sculas el texto antes de normalizar\n",
    "    print(\"antes: \", texto)\n",
    "    texto = normaliza_texto(texto.lower())\n",
    "    print(\"despu√©s:\",texto)\n",
    "    return texto\n",
    "    \n",
    "    \n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(docs)\n",
    "print(vec_tfidf.get_feature_names_out())\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio:\n",
    " - ### Obtener la matriz Documento-T√©rmino del archivo text_3.json\n",
    " - ### Mejorar el preprocesamiento del texto: convertir a min√∫sculas, separar las oraciones, normalizar el texto, y separar los tokens adecuadamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    print(\"antes: \", texto)\n",
    "\n",
    "    #TODO: Mejorar el preprocesamiento, convertir a min√∫sculas, separar las oraciones, normalizar el texto, y separar los tokens\n",
    "\n",
    "\n",
    "\n",
    "    print(\"despu√©s:\",texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(docs)\n",
    "print(vec_tfidf.get_feature_names_out())\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 T√©cnicas para reducir las caracter√≠sticas (**features**) textuales\n",
    "#### - **Stemming**.  Corta los finales de las palabras de acuerdo a un conjunto de reglas\n",
    "#### - **Lematizaci√≥n**. Reducir a la forma normal de la palabra, es decir, verbos a infinitivo; adjetivos a masculino, singular;  sustantivos a singular.\n",
    "#### - **Remover StopWords**. Eliminar palabras que no se consideran de contenido, es decir, que no aportan a la sem√°ntica como preposiciones: a, ante, de, desde, etc.; conjunciones entre otras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "### - El stemming es el proceso de reducir las palabras a su ra√≠z o base sin considerar su contexto gramatical.\n",
    "### - A menudo, la ra√≠z obtenida no es una palabra real, pero es √∫til para tareas de procesamiento de lenguaje natural.\n",
    "### - Algoritmos populares de stemming incluyen el **algoritmo de Porter** y el **algoritmo de Snowball**.\n",
    "### - Herramientas como **NLTK** incluyen implementaciones de estos algoritmos para varios idiomas.\n",
    "### - A diferencia de la lematizaci√≥n, el stemming es m√°s r√°pido pero menos preciso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "print(stemmer.stem(\"mesas\"))\n",
    "print(stemmer.stem(\"mesa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematizaci√≥n \n",
    "\n",
    "### - La lematizaci√≥n es el proceso de reducir las palabras a su forma base o \"lema\", considerando su contexto gramatical.\n",
    "### - A diferencia del stemming, la lematizaci√≥n analiza la estructura ling√º√≠stica para obtener una forma m√°s precisa.\n",
    "### - Ejemplo: \"corriendo\" y \"corri√≥\" se reducen a \"correr\" (verbo en infinitivo).\n",
    "### - Herramientas como **spaCy** ofrecen lematizaci√≥n en varios idiomas.\n",
    "### - **NLTK** y **TextBlob** tambi√©n permiten realizar lematizaci√≥n.\n",
    "### - **spaCy** es reconocido por su eficiencia y precisi√≥n al considerar el contexto gramatical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Instalaci√≥n de los modelos de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "doc = nlp(\"me com√≠ unas galletas con mis amigos, pero despu√©s Juan se comi√≥ la √∫ltima que quedaba\")\n",
    "print(doc.text)\n",
    "print(\"=\"*20)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.dep_, token.lemma_)\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "- #### Las **stopwords** son palabras muy comunes en un idioma que suelen tener poco valor sem√°ntico o informativo en el an√°lisis de texto. \n",
    "- #### En espa√±ol, incluyen incluyen art√≠culos, preposiciones y pronombres como  \"el\", \"la\", \"de\", \"y\", \"que\", \"es\", entre otros. \n",
    "- #### Estas palabras suelen eliminarse para reducir el ruido, se eliminan durante el preprocesamiento de texto para centrarse en t√©rminos m√°s relevantes, facilitando tareas como la clasificaci√≥n,  an√°lisis de sentimientos, b√∫squeda y miner√≠a de texto.\n",
    "- #### Herramientas como **NLTK**, **spaCy** y **scikit-learn** ofrecen listas predeterminadas de stopwords.\n",
    "- #### Las stopwords se adaptan al idioma y a la tarea, por lo que a veces es √∫til personalizar la lista seg√∫n el contexto del problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga de las listas de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar m√°s palabras a esta lista si es necesario\n",
    "\n",
    "print(_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"El cielo es azul,  5342534523534 !!!!!?????????!\",\n",
    "\"El sol es,brillante  ?????!!!!!!\",\n",
    "\"El sol en el, cielo,es muy  brillante\",\n",
    "\"Podemos ver el sol brillante, el sol brillante\",\n",
    "\"JUAN LUIS,  ama MAR√≠A !!!!!\",\n",
    "\"Mar√≠a ama a Luis\",\n",
    "\"la estrella de la ma√±ana y la estrella del atardecer es la misma estrella\",\n",
    "\" El Lucero del alba se le llama cuando es visible en el cielo al amanecer. El Lucero de la tarde es visible en el firmamento al atardecer.\"\n",
    "]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar m√°s palabras a esta lista si es necesario\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a min√∫sculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor sem√°ntico al texto\n",
    "    print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "\n",
    "    print(\"despu√©s:\",texto)\n",
    "    \n",
    "    return texto\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(docs)\n",
    "print(vec_tfidf.get_feature_names_out())\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir los ejemplos de prueba al mismo espacio de representaci√≥n de datos del conjunto de entrenamiento que aprendi√≥ el modelo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = [\n",
    "\"El sol es muy brillante el d√≠a de hoy\",\n",
    "\"Podemos ver el cielo azul\",\n",
    "\"JUAN LUIS   se fue de paseo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El m√©todo transform convierte la entrada al espacio de representaci√≥n del texto modelado con los datos de entrenamiento\n",
    "doc_test_tfidf = vec_tfidf.transform(doc_test)\n",
    "print(doc_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecci√≥n de los datos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tama√±o de la matriz de prueba: (ejemplos, tama√±o de features) \", doc_test_tfidf.shape)\n",
    "n_sample=0\n",
    "print(\"\\ntexto original: \")\n",
    "print(doc_test[n_sample])\n",
    "#type(vec_tfidf.inverse_transform(X_test_tfidf[n_sample]))\n",
    "\n",
    "print(\"\\ntexto del espacio de representaci√≥n: \")\n",
    "print(\" \".join(vec_tfidf.inverse_transform(doc_test_tfidf[n_sample])[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Preparaci√≥n de los conjuntos de datos para el entrenamiento y evaluaci√≥n</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Cargar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "['@USUARIO Lo que tu no respetas es al pa√≠s HDP', 'no s√© c√≥mo pedirle a la se√±ora que me ayuda a desacomodar los libros que no use mi librero como un mueble cualquiera sin parecer una loca', '¬øC√≥mo hacer entender en el grupo de la familia que me cagan las putas cadenas de oraci√≥n, im√°genes de Piolin y videos \"chistosos\"?', 'Jaja, perd√≥n por esta vida loca. Nos vemos pronto, bestia.', 'Pero que putas hacen esos argentinos hablando como espa√±oles.¬ø?', 'S√≥lo a las PUTAS MOCOSAS GORDAS Y feas les gusta ese aleman xdxd Si a mi me das un puto perro asco, imag√≠nate el... <URL>', '@USUARIO Emilio Gamboa? EPN? Manlio?? Olv√≠denlo son tantos los hdp q imposible saber cu√°l fu√©?  üòúüòúüòú', '¬øPorqu√© no te fuiste a Cruz Azul?  La Rebel ya te quiere partir la madre por muerto. Yo ah√≠ nom√°s te aviso,un abrazo.', 'Chingue a su madre pinche profesor@ de @USUARIO', 'Este chavo que est√° cort√°ndose el cabello me hace ojitos! Y yo tan joto que no le hago caso ü§¶üèæ\\u200d‚ôÇÔ∏è', 'Que loca la Rogelia no queriendo compartir la chiluda #MasterChefMx', '@USUARIO Sugiero haya cadeneros en las entradas del metro ¬°basta de viajar con malandros! ¬°hasta prostitutas trabajan en el metro! No mamar.', 'Padres de familia del Colegio Rebsamen piden que Putos Todos.', 'En enero ser√© madrina, y bueno, ser√© una madrina luchona Jajajaja üë∂üèªüë©üèªüíô', 'A veces la gente tiene actitudes tan tontas que me sorprende que se llamen \"adultos\".', 'No tolero a los hombres q dicen ‚Äúim gonna wife her‚Äù o ‚Äúla voy a hacer mi mujer‚Äù.. o sea.. que putas? Ni q fuera tu COSA para hacerla TUYOüò§üò°', 'Que como dec√≠a mi madre, bailando todo se arregla ...üíÉüèºüíÉüèºüòÉ siempre hay que encontrarle lo bueno a la vida üòÉ #FelizMiercoles  üå∫', 'me caga que de frente me veo s√∫per ancha, de lado maybe me puedo ver delgada aveces pero de frente siempre me veo gorda', '@USUARIO Siempre les digo a las j√≥venes que trabajen para evitar que las mantenga un mazacote que les cuente el dinero. Tontas.', 'Estoy segura que la gorda le quer√≠a dar la mordida a tu pastel ü§£üôä', 'de verdad que chinguen a su madre todos hoy me cae', 'Todas estan feas menos la que me gusta.', 'As√≠ estas hijo d tu puta madre...cuando ganamos la final como Cagan la verga putos...', 'Ver un \"cl√°sico\" de Liga pitera? No mamar hay ufc y el regreso GSP. Hay niveles', '@USUARIO Hay cosas tan tontas pero q son taaaaaaaan llamativas q las debes tener jajajaja üòå', '@USUARIO me pareci√≥ jocoso/triste/surreal/irrespetuoso/cagado, justo como esta agridulce gran chinampa', 'Uhummm yo soy la gata viajera!!!üò®üïµÔ∏è\\u200d‚ôÇÔ∏èü§§üéµüé∂üé∂üé∂ <URL>', 'Se pasaron de verga la cague muy feo en el gold xdd izi win era', 'Me sirve m√°s limpiar mis lentes con papel de ba√±o que con esos putos trapos especiales que venden.', 'Seguro era gorda o ten√≠a pintadas las cejas']\n",
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Lee el archivo de datos: el formato es una linea es un ejemplo de entrenamiento.\n",
    "# dataset = pd.read_json(\"./data/data_emotions_es.json\", lines=True)\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "\n",
    "\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "\n",
    "#primeros 30 textos\n",
    "print(dataset[\"text\"].to_list()[:30])\n",
    "\n",
    "print(dataset.klass.value_counts())\n",
    "\n",
    "# Extracci√≥n de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "\n",
    "# Extracci√≥n de las etiquetas o clases de entrenamiento\n",
    "\n",
    "Y = dataset['klass'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificar las categor√≠as utilizando un esquema de codificaci√≥n ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases codificadas:\n",
      "['aggressive' 'nonaggressive']\n",
      "Clases codificadas:\n",
      "['aggressive' 'nonaggressive' 'nonaggressive' 'nonaggressive'\n",
      " 'nonaggressive'] [0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificaci√≥n ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.classes_)\n",
    "\n",
    "n=5\n",
    "print(\"Clases codificadas:\")\n",
    "print(Y[:n], Y_encoded[:n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decodificar las categor√≠as en  codificaci√≥n ordinal a la categor√≠a original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aggressive', 'aggressive', 'nonaggressive'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arreglo con  resultados de las clases en la categor√≠a codificada. \n",
    "le.inverse_transform([0, 0 , 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preparaci√≥n de los conjuntos de datos  (datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hay  dos formas principales de evaluaci√≥n de modelos: partici√≥n y validaci√≥n cruzada:\n",
    "\n",
    "### 1. **Evaluaci√≥n por partici√≥n (separaci√≥n de datasets en train-test)**:\n",
    "   - #### Dividir el conjunto de datos en dos subconjuntos: uno para entrenar (train) y otro para probar (test).\n",
    "   - #### Proceso simple y r√°pido, adecuado para conjuntos de datos grandes.\n",
    "   - #### Riesgo de que la evaluaci√≥n dependa de c√≥mo se haya dividido el conjunto, lo que puede generar un rendimiento sesgado.\n",
    "   - #### Se utiliza un porcentaje fijo de datos para entrenar y el resto para evaluar, como 70%/30% o 80%/20%.\n",
    "   \n",
    "### 2. **Evaluaci√≥n por validaci√≥n cruzada**:\n",
    "   - #### El conjunto de datos se divide en m√∫ltiples subconjuntos (folds) y se entrena el modelo varias veces, usando cada fold como conjunto de prueba y el resto como entrenamiento.\n",
    "   - #### M√°s robusta y fiable, ya que la evaluaci√≥n se basa en m√∫ltiples particiones, lo que reduce el sesgo.\n",
    "   - #### Ayuda a obtener una estimaci√≥n m√°s precisa del rendimiento del modelo en datos no vistos.\n",
    "   - #### La variante m√°s com√∫n es la **k-fold cross-validation**, donde los datos se dividen en *k* subconjuntos.\n",
    "\n",
    "\n",
    "      <img src=\"figs/fig_cross-validation.jpg\" width=\"800\">\n",
    "\n",
    "\n",
    "###### Fuente: https://es.wikipedia.org/wiki/Validaci√≥n_cruzada\n",
    "\n",
    "### **Ambos m√©todos tienen ventajas y desventajas, siendo la validaci√≥n cruzada m√°s precisa pero costosa en t√©rminos computacionales.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separaci√≥n de datos por partici√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas: test_size = 0.2 (20%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify= Y_encoded, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Graciaspor incitar al odio y a los feminicidios. Aprende: NUNCA perdemos nuestros derechos. #YoSoyMara',\n",
       "        'No hay mayor mam√° m√°s  luchona y cabrona que Cristiano Ronaldo ‚òù #halamadrid',\n",
       "        'Somos bien hdp con ella, la verdad. A veces hasta me siento mal de ser as√≠, pero todos dicen que se lo gana. üòÇüòå',\n",
       "        '@USUARIO Que cabron ver La publicidad de Megadeth por toda Baja California, recuerdo pasar por un puente de Mexicali y ver a todo color a mi banda!ü§òüèº',\n",
       "        'Ahora si te la mamaste :p pero se respeta tu opini√≥n. Tambien quien sabe en que plan ibas, o que esperabas. A mi me encant√≥',\n",
       "        'yo soy una buena persona SIN MAMAR ok? as√≠ que yo no se porqu√© pitos me pasan cosas tan de la verga',\n",
       "        'En ning√∫n momento se me sali√≥ el joto que llevo dentro jajaja. Quedamos en comenzar a entrenar juntos.',\n",
       "        'Se me ocurri√≥ una idea muy loca! La neta no se si se haga realidad pero bueh!',\n",
       "        '@USUARIO T√∫ tambi√©n estorbabas cu√°ndo eras una gorda, por eso el Cuauh te dio un vergazo para que te abrieras a la verga',\n",
       "        'La gente delgada que dice que est√° gorda...'], dtype=object),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1]),\n",
       " array(['nonaggressive', 'nonaggressive', 'nonaggressive', 'nonaggressive',\n",
       "        'nonaggressive', 'nonaggressive', 'nonaggressive', 'nonaggressive',\n",
       "        'aggressive', 'nonaggressive'], dtype=object),\n",
       " 4105,\n",
       " 1027)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostrar los primeros N ejemplos con sus clases\n",
    "samples=10\n",
    "X_train[:samples], Y_train[:samples], le.inverse_transform(Y_train[:samples]), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separaci√≥n de datos para validaci√≥n cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "train [num. clases]-  [1181 2924]   |   test [num. clases]-  [296 731]\n",
      "k = 2\n",
      "train [num. clases]-  [1181 2924]   |   test [num. clases]-  [296 731]\n",
      "k = 3\n",
      "train [num. clases]-  [1182 2924]   |   test [num. clases]-  [295 731]\n",
      "k = 4\n",
      "train [num. clases]-  [1182 2924]   |   test [num. clases]-  [295 731]\n",
      "k = 5\n",
      "train [num. clases]-  [1182 2924]   |   test [num. clases]-  [295 731]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#tama√±o de los k-folds\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)  \n",
    "for k, (index_train, index_test) in enumerate(skf.split(X, Y_encoded), start=1):\n",
    "    \n",
    "    # index_train y index_test obtienen los √≠ndices de las muestras para procesar\n",
    "    X_train_cv, X_test_cv = X[index_train], X[index_test]\n",
    "\n",
    "    Y_train_cv, Y_test_cv = Y_encoded[index_train], Y_encoded[index_test]\n",
    "\n",
    "    print(\"k = {}\".format(k))\n",
    "    print('train [num. clases]-  {}   |   test [num. clases]-  {}'.format(\n",
    "        np.bincount(Y_train_cv), np.bincount(Y_test_cv) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"white\">Ejercicio: </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Preprocesar los datos para la creaci√≥n de la matriz de documento-t√©rmino, usar un pesado TF-IDF, y unigramas\n",
    "\n",
    "###  2. Preparar los conjuntos de datos para el entrenamiento con un partici√≥n train-test: dividir 70% para entrenamiento y 30% para test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cargar el conjunto de datos \"data_aggressiveness_es.json\" \n",
    "\n",
    "\n",
    "# TODO: Preprocesar los datos: convertir a min√∫sculas, borrar puntuaci√≥n, etc.\n",
    "\n",
    "\n",
    "# TODO: Codificar las clases si no son categor√≠as ordinales.\n",
    "\n",
    "\n",
    "# TODO: Separar los datos 70% entrenamiento y 30% test\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Contruir la matriz de documento-t√©rmino  para el conjunto de entrenamiento y para el conjunto de test \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
