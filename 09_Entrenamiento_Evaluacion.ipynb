{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/fig-diagrama-clasificador.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar al clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Red Neuronal Multicapa\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"600\" style=\"background-color:white;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"./data/data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Codificar las categorías (clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparar los conjuntos de datos  (datasets) para entrenamiento y para probar el rendimiento del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Crear Matriz Documento-Término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor semántico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"después:\",texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Crear el clasificador: Crear la clase MLP_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generación de números aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # Calcular el límite de la distribución uniforme\n",
    "    limit = np.sqrt(6 / (input_size + output_size))\n",
    "    # Generar la matriz de pesos con distribución uniforme en el rango [-limit, limit]\n",
    "    W = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "    return W\n",
    "\n",
    "\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los índices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y según los índices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=32, learning_rate=0.5, random_state=42):\n",
    "\n",
    "        seed(random_state)\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el número de épocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tamaño del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # definir las capas\n",
    "        self.W1 = xavier_initialization(num_neuronas_ocultas, num_entradas)  # Pesos entre capa de entrada y capa oculta\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))   # Bias de la capa oculta\n",
    "        self.W2 = xavier_initialization(num_salidas, num_neuronas_ocultas)  # Pesos entre capa oculta y capa de salida\n",
    "        self.b2 = np.zeros((1, num_salidas)) # Bias de la capa de salida\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: implementar el forward pass\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagación hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "        self.X = X\n",
    "        self.z_c1 = 0\n",
    "        # TODO: Calcular la activación de la capa oculta usando la función sigmoide\n",
    "        self.a_c1 = 0\n",
    "        # TODO: Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "        self.z_c2 = 0\n",
    "        # TODO: Calcular la activación de la capa de salida usando la función sigmoide\n",
    "        y_pred = 0  # Activación capa de salida\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. Cálculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular el error cuadrático medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        # TODO: implementar el backward pass\n",
    "        # calcular los gradientes para la arquitectura de la figura anterior\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagación hacia atrás (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = 0 # Derivada del error respecto a la predicción con  N ejemplos\n",
    "        # TODO: Calcular la derivada de la activación de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = 0\n",
    "        # TODO: Calcular delta de la capa de salida\n",
    "        delta_c2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = 0\n",
    "        # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = 0\n",
    "\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = 0\n",
    "        self.dE_db2 =0\n",
    "        self.dE_dW1 =  0\n",
    "        self.dE_db1 =  0\n",
    "\n",
    "\n",
    "    def update(self):  # Ejecución de la actualización de paramámetros\n",
    "        # TODO: implementar la actualización de los pesos y el bias\n",
    "        #----------------------------------------------\n",
    "        # Actualización de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = 0\n",
    "        self.b2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzación de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la función de error respecto a los pesos de la capa 1\n",
    "        self.W1 = 0\n",
    "        self.b1 = 0\n",
    "\n",
    "    def predict(self, X):  # Predecir la categoría para datos nuevos\n",
    "        # TODO: implementar la predicción \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "        for epoch in range(self.epochs):\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                self.backward() # cálculo de los gradientes\n",
    "                self.update() # actualización de los pesos y bias\n",
    "\n",
    "                # Imprimir el error cada N épocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"Época {epoch}, Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Entrenar la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_tr = X_tfidf.toarray()\n",
    "Y_tr = Y_train[:, np.newaxis] # Agregar una dimensión adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "num_entradas= X_tr.shape[1] # tamaño de la matriz Documento-Término\n",
    "num_neuronas_ocultas = 128\n",
    "num_salidas = 1\n",
    "epochs = 110 \n",
    "batch_size = 32\n",
    "learning_rate = 0.5\n",
    "random_state = 33\n",
    "\n",
    "clasificador_mlp = MLP_TODO(num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size, learning_rate, random_state)\n",
    "\n",
    "# Entrenamos al clasificador\n",
    "clasificador_mlp.train(X_tr, Y_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplos_nuevos = [\"Que se puede esperar de este perro ladrón\"]\n",
    "# Suponer que se cuenta con el objeto vec_tfidf entrenado con el vocabulario del conjunto de entrenamiento\n",
    "X_ejemplos_tfidf = vec_tfidf.transform(ejemplos_nuevos)\n",
    "X_ejemplos_tfidf = X_ejemplos_tfidf.toarray()\n",
    "print(X_ejemplos_tfidf)\n",
    "\n",
    "y_pred_nuevo = clasificador_mlp.predict(X_ejemplos_tfidf)\n",
    "y_pred_nuevo = y_pred_nuevo.flatten()\n",
    "print(le.inverse_transform(y_pred_nuevo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predecir los datos del conjuntos de prueba con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "X_t = X_test_tfidf.toarray()\n",
    "Y_t = Y_test[:, np.newaxis] # Agregar una dimensión adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "y_pred_test = clasificador_mlp.predict(X_t)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspección de los resultados de los primeros N ejemplos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"textos: \", X_test[:5])\n",
    "print(\"clase esperada: \",Y_test[:5])\n",
    "print(\"clase predicha: \", y_pred_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar la predicción de la clase original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test[:10])\n",
    "print(y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llevar a la misma forma la salida de las predicciones\n",
    "y_pred_test = y_pred_test.flatten()\n",
    "\n",
    "print(Y_test[:10])\n",
    "print(y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obten las primeras N predicciones\n",
    "pred =  y_pred_test[:5] \n",
    "pred_ori = le.inverse_transform(pred)\n",
    "pred, pred_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluando el desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de Evaluación\n",
    " - #### Las métricas precisión, recall y F1 son fundamentales para evaluar el rendimiento de un clasificador\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_precision-recall.png\" width=\"300\">\n",
    "\n",
    "##### Fuente: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_matriz-confusion.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP=True Positive\n",
    "\n",
    "TN=True Negative\n",
    "\n",
    "FP=False Positive (Error tipo I: ejemplo, se considera que el paciente está enfermo, pero en realidad está sano)\n",
    "\n",
    "FN=False Negative ( Error tipo II: ejemplo, se considera que el paciente está sano, pero en realidad está enfermo)\n",
    "\n",
    "\n",
    "$$ Accuracy = \\frac{total~ TP + total~TN}{total~muestras} $$\n",
    "\n",
    "$$ Precision_c = \\frac{ TP_c}{TP_c + FP_c} $$\n",
    "\n",
    "$$ Recall_c = \\frac{ TP_c}{TP_c + FN_c} $$\n",
    "\n",
    "$$ F1-score_c= 2 \\times \\frac{ Precision_c \\times Recall_c}{Precision_c + Recall_c} $$\n",
    "\n",
    "$$ macro-F1-score= \\frac{ 1 }{|Clases|} \\sum{F1-score_c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para la clase 0, la precisión es la siguiente\n",
    "tp= 82\n",
    "fp = 31+11+33\n",
    "tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print(\"P=\", precision_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"R=\", recall_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"F1=\", f1_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"Acc=\", accuracy_score(Y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección del desempeño por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
